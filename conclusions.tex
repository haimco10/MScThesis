\chapter{Summary and Conclusions}
%To conclude
We proposed a new framework for online multi-task learning, where  learners share a single annotator. 
We presented number of (SHAMPO) algorithms that work in this settings : The First Order , First Order Aggressive, 
First Order Adaptive , First Order with Prior,  Second Order and Aggressive Second Order. 
Mistake bound analysis was done for most of the proposed algorithms. 
Then, we showed how learning in such model can be used to learn in contextual-bandits setting 
with few types of feedback. Empirical results show that our algorithms, with wise choosing of parameters, 
does better than only random uniform choice of feedback, for the same price. 
It focuses the annotator on the harder instances, and is improving performance in various tasks and settings. 

Further work can be done  to improve those algorithms. We have shown that there exist an optimal value of 
$b$ that may vary for different multitask problems. We fed the algorithm with the parameter $b$, yet we did 
not talk about how to find such optimal value. Now, after we shown that such parameter exists, 
we can try to derive an algorithm that finds this optimal value for every dataset.  
We can also investigate ways to generate good prior distribution for the SHAMPO with prior algorithm. 
Another way can be deriving another distribution that select the task to be queried at every step,
 not only by the margin and the 
number of queried mistakes (as in the adaptive version) up to the same step, but also consider the  relation 
between number of queries, and number of updates for every task, since for a task that query all the time
and doesn't improve the results, we may want to reduce the queries rate. 
Another improvement can be done if we assume a relation between tasks and at each step we update not 
only the queried tasks, but also similar tasks or all the tasks, exploiting the relation between the tasks .

% In this paper we extended the methods of partial feedback and selective sampling algorithms to the 
%multi-task case and  introduced MUTSEA, a new multi-task selective sampling algorithm. 
%The algorithm doesn't force a  same space for all of the tasks inputs and the bound on its expected 
%learning mistakes, shows that the bound  becomes tighter for a certain value of the parameter $b$. 
%Our experiments show that this is indeed the case, and the minimal error is obtained for a certain optimal 
%point. Moreover, the experiments shows that this optimal point can be approximately found,  using only the 
%partial feedback. We plan to study and develop this algorithm into the multi-class direction and try to make 
%the algorithm adaptive such that its single parameter could be adapted iteratively to the data.
