\chapter{Analysis of the LASER algorithm}
\label{chap:LASER_analysis}

In this chapter we analyze the performance of the \texttt{LASER} algorithm in the worst-case
setting in six steps. First, state a technical lemma (\lemref{lem:technical}) that is used in
the second step (\thmref{thm:basic_bound}), in which we bound the regret with a quantity
proportional to
$\sum_{t=1}^{T}\vxti{t}D_{t}^{-1}\vxi{t}$. Third,
in \lemref{lem:bound_1} we bound each of the summands with two terms,
one logarithmic and one linear in the eigenvalues of the matrices
$D_{t}$. In the fourth (\lemref{operator_scalar}) and fifth (\lemref{eigen_values_lemma})
steps we bound the eigenvalues of $D_{t}$ first for scalars
and then extend the results to matrices. Finally, in \corref{cor:main_corollary} we put
all these results together and get the desired bound.


\begin{lemma}
For $t \geq 1$ the following statement holds,
\begin{align*}
D'_{t-1}D_{t}^{-1}\vxi{t}\vxti{t}D_{t}^{-1}D'_{t-1}
+D'_{t-1}\left(D_{t}^{-1}D'_{t-1}+c^{-1}\mi\right)-D_{t-1}^{-1}
\preceq0~,
\end{align*}
where as defined in \eqref{D_prime} we have
\(
D'_{t-1}=\left(\mi+c^{-1}D_{t-1}\right)^{-1}~.
\)
\label{lem:technical}
\end{lemma}
% \begin{lemma}
% For all $t$ the following statement holds,
% \begin{align*}
% &\left(\mi+c^{-1}D_{t-1}\right)^{-1}D_{t}^{-1}\vxi{t}\vxti{t}D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}-D_{t-1}^{-1}\\
% &+\left(\mi+c^{-1}D_{t-1}\right)^{-1}\left[D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}+c^{-1}\mi\right]\preceq0
% \end{align*}
% \label{lem:technical}
% \end{lemma}
%The proof appears in \secref{proof_lemma_technical}.
\begin{proof}
We first use the Woodbury identity to get the following two identities
\begin{align*}
&D_{t}^{-1}=\left[\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}+\vxi{t}\vxti{t}\right]^{-1}=D_{t-1}^{-1}+c^{-1}\mi-\frac{\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}\vxti{t}
\left(D_{t-1}^{-1}+c^{-1}\mi\right)}{1+\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}}\\
&\left(\mi+c^{-1}D_{t-1}\right)^{-1}=\mi-c^{-1}\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}~.
\end{align*}
Multiplying both identities with each other we get, %For the first part of the lemma we have,
\begin{align}
 & D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}
% =  \left[D_{t-1}^{-1}+c^{-1}\mi-\frac{\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)}{1+\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}}\right]\nonumber\\
% &\left[\mi-c^{-1}\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}\right]
 = D_{t-1}^{-1}-\frac{\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}\vxti{t}D_{t-1}^{-1}}{1+\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}}~,\label{identity1}
\end{align}
and, similarly, we multiply the identities in the other order and get,%for the second part of the lemma we have,
\begin{align}
&  \left(\mi+c^{-1}D_{t-1}\right)^{-1}D_{t}^{-1}
%= \left[\mi-c^{-1}\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}\right]\nonumber\\
%& \left[D_{t-1}^{-1}+c^{-1}\mi\vphantom{\frac{\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)}{1+\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}}}
% -\frac{\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)}{1+\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}}\right]
 =D_{t-1}^{-1}-\frac{D_{t-1}^{-1}\vxi{t}\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)}{1+\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}}~.
\label{identity2}
\end{align}
Finally, from \eqref{identity1} we get,
\begin{align*}
&\left(\mi+c^{-1}D_{t-1}\right)^{-1}D_{t}^{-1}\vxi{t}\vxti{t}D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}-D_{t-1}^{-1}\\
&\quad+\left(\mi+c^{-1}D_{t-1}\right)^{-1}\left[D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}+c^{-1}\mi\right]\\
=~&
 \left(\mi+c^{-1}D_{t-1}\right)^{-1}D_{t}^{-1}\vxi{t}\vxti{t}D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}-D_{t-1}^{-1}\\
&\quad
+\Bigg[
\mi-c^{-1}\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}
\Bigg]
\Bigg[
D_{t-1}^{-1}+c^{-1}\mi\vphantom{\frac{
\left(
D_{t-1}^{-1}+c^{-1}\mi
\right)
\vxi{t}\vxti{t}
\left(
D_{t-1}^{-1}+c^{-1}\mi
\right)
}{1+\vxti{t}
\left(
D_{t-1}^{-1}+c^{-1}\mi
\right
)\vxi{t}}}
-\frac{\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}\vxti{t}D_{t-1}^{-1}}{1+\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}}\Bigg]~.
\end{align*}
We further develop the last equality and use \eqref{identity1} and
\eqref{identity2} in the second equality below,
\begin{align*}
 =~ &
 \left(\mi\!+\!c^{-1}D_{t-1}\right)^{-1}\!D_{t}^{-1}\vxi{t}\vxti{t}D_{t}^{-1}\left(\mi\!+\!c^{-1}
 D_{t-1}\right)^{-1}\!-\!D_{t-1}^{-1}+D_{t-1}^{-1}-\frac{D_{t-1}^{-1}\vxi{t}\vxti{t}D_{t-1}^{-1}}{1+\vxti{t}
\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}}\\
 =~ &
 \left[
D_{t-1}^{-1}-\frac{D_{t-1}^{-1}\vxi{t}\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)}
 {1+\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}}\right]\vxi{t}\vxti{t} \Bigg[D_{t-1}^{-1}\vphantom{\frac{\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}\vxti{t}D_{t-1}^{-1}}
 {1+\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}}}
-\frac{\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}\vxti{t}D_{t-1}^{-1}}
 {1+\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}}
\Bigg]\\
&
-\frac{D_{t-1}^{-1}\vxi{t}\vxti{t}D_{t-1}^{-1}}{1+\vxti{t}\left(D_{t-1}^{-1}+c^{-1}
\mi\right)\vxi{t}}\\
 =~ & -\frac{\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}D_{t-1}^{-1}\vxi{t}\vxti{t}
 D_{t-1}^{-1}}{\left(1+\vxti{t}\left(D_{t-1}^{-1}+c^{-1}\mi\right)\vxi{t}\right)^{2}}~~\preceq~~0 ~.
\end{align*}
\QED
 \end{proof}

We next bound the cumulative loss of the algorithm.
\begin{theorem}
\label{thm:basic_bound}
Assume that the labels are bounded $\sup_t \vert \yi{t} \vert \leq Y$ for some
$Y\in\reals$. Then the following bound holds,
\begin{align}
%\sum_{t=1}^{T}\left(y_{t}-\hat{y}_{t}\right)^{2}
L_T(\texttt{LASER})&\leq\min_{\vui{1},\ldots,\vui{T}}\left[
L_T(\{\vui{t}\})
+c V_T(\{\vui{t}\})%\sum_{t=1}^{T-1}\left\Vert   \vui{t+1}-\vui{t}\right\Vert ^{2}
+b\left\Vert
    \vui{1}\right\Vert ^{2}
%+
%\sum_{t=1}^{T}\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}
\right]+Y^{2}\sum_{t=1}^{T}\vxti{t}D_{t}^{-1}\vxi{t} ~.\label{bound_1}
\end{align}
\end{theorem}
\begin{proof}
Fix $t$.
%A long algebraic manipulation, given in \secref{algebraic_manipulation}
\begin{eqnarray*}
 &  & \left(y_{t}-\hat{y}_{t}\right)^{2}+\min_{\vui{1},\ldots,\vui{t-1}}Q_{t-1}\left(\vui{1},\ldots,\vui{t-1}\right)-\min_{\vui{1},\ldots,\vui{t}}Q_{t}\left(\vui{1},\ldots,\vui{t}\right)\\
 &=  & \left(y_{t}-\hat{y}_{t}\right)^{2}-\veti{t-1}D_{t-1}^{-1}\vei{t-1}+f_{t-1}+\veti{t}D_{t}^{-1}\vei{t}-f_{t}\\
 & = & \left(y_{t}-\hat{y}_{t}\right)^{2}-\veti{t-1}D_{t-1}^{-1}\vei{t-1}\\
 &  &
 +\left(\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}+y_{t}\vxi{t}\right)^{\top}D_{t}^{-1}\left(\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vphantom{\left(\mi+c^{-1}D_{t-1}\right)^{-1}}\vei{t-1}+y_{t}\vxi{t}\right)\\
&&
+\veti{t-1}\left(c\mi+D_{t-1}\right)^{-1}\vei{t-1}-y_{t}^{2} ~,
\end{eqnarray*}
where the last equality follows from \eqref{e} and \eqref{f}. We proceed to develop the
last equality,
\begin{eqnarray*}
&=  & \left(y_{t}-\hat{y}_{t}\right)^{2}-\veti{t-1}D_{t-1}^{-1}\vei{t-1}\\
 &  & +\veti{t-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}D_{t}^{-1}\left
 (\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}\\
 &  & +2y_{t}\vxti{t}D_{t}^{-1}
 \left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}\\
 &  &     +y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t}
  +\veti{t-1}\left(c\mi+D_{t-1}\right)^{-1}\vei{t-1}-y_{t}^{2}\\
 & = & \left(y_{t}-\hat{y}_{t}\right)^{2}
+\veti{t-1}
\Bigg(-D_{t-1}^{-1}
+\left(\mi+c^{-1}D_{t-1}\right)^{-1}
 D_{t}^{-1}
\Big(\mi +c^{-1}D_{t-1}
\Big)^{-1}\\
&&+c^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}
\Bigg)
 \vei{t-1}+2y_{t}\vxti{t}D_{t}^{-1}\left(\mi\!+\!c^{-1}D_{t-1}\right)^{-1}\vei{t-1} \!+\!y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t}\!-\!y_{t}^{2}\\
 &  =&
 \left(y_{t}-\hat{y}_{t}\right)^{2}+\veti{t-1}
\Bigg(-D_{t-1}^{-1}+
\Big(\mi\vphantom{\left[D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}+c^{-1}\mi\right]}
+c^{-1}D_{t-1}
\Big)^{-1}
\left[D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}+c^{-1}\mi\right]
\Bigg)\vei{t-1}\\
 &  & +2y_{t}\vxti{t}D_{t}^{-1}\left(\mi\!+\!c^{-1}D_{t-1}\right)^{-1}\vei{t-1}\!+\!y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t}\!-\!y_{t}^{2}~.
\end{eqnarray*}
% \begin{align}
%   & \left(y_{t}-\hat{y}_{t}\right)^{2}+\min_{\vui{1},\ldots,\vui{t-1}}Q_{t-1}\left(\vui{1},\ldots,\vui{t-1}\right)\nonumber\\
%   & \quad -\min_{\vui{1},\ldots,\vui{t}}Q_{t}\left(\vui{1},\ldots,\vui{t}\right)\label{step1}\\
%   =~& \left(y_{t}-\hat{y}_{t}\right)^{2}+\veti{t-1}\left(-D_{t-1}^{-1}\vphantom{\left[D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}+c^{-1}\mi\right]}+\right.\nonumber\\ &\left.\left(\mi+c^{-1}D_{t-1}\right)^{-1}\left[D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}+c^{-1}\mi\right]\right)\vei{t-1}\nonumber\\
%   & +2y_{t}\vxti{t}D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}+y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t}-y_{t}^{2}~.\nonumber
% \end{align}
%\begin{align}
%&\left(y_{t}-\hat{y}_{t}\right)^{2}+\min_{\vui{1},\ldots,\vui{t-1}}Q_{t-1}\left(\vui{1},\ldots,\vui{t-1}\right)\nonumber\\
%&~~~~~~~~~~~~ -\min_{\vui{1},\ldots,\vui{t}}Q_{t}\left(\vui{1},\ldots,\vui{t}\right)\nonumber\\
%=&\left(y_{t}-\hat{y}_{t}\right)^{2}%+ \nonumber\\ &
%+2y_{t}\vxti{t}D_{t}^{-1}D'_{t-1}\vei{t-1}\nonumber\\
%&\!+\!\veti{t-1}\!\Bigg[ \!-\!D_{t-1}^{-1} %\nonumber\\ &
%\!+\!\!
%%\mi+\frac{1}{c}D_{t-1}
%D'_{t-1}
%\!\left(D_{t}^{-1}
%D'_{t-1}
%%\mi+\frac{1}{c}D_{t-1}
%\!\!%\nonumber\\&
%\!+\!c^{-1}\mi\right)\!\!\Bigg]\!\vei{t-1}\nonumber\\
%&+y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t}-y_{t}^{2}~.
%\label{step1}
%\end{align}
% Substituting the specific value of the predictor
% $\hat{y}_{t}=\vxti{t}D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}$
% from \eqref{my_predictor}, we get that \eqref{step1} equals to,
% \begin{align}
%   & \hat{y}_{t}^{2}+y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t}+\veti{t-1}\left(-D_{t-1}^{-1}\vphantom{\left[D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}+c^{-1}\mi\right]}+\right.\nonumber\\ & \left.\left(\mi+c^{-1}D_{t-1}\right)^{-1}\left[D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}+c^{-1}\mi\right]\right)\vei{t-1}\nonumber\\
% & = \veti{t-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}D_{t}^{-1}\vxi{t}\vxti{t}D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}\nonumber\\
%  & +\veti{t-1}\left(-D_{t-1}^{-1}+\left(\mi+c^{-1}D_{t-1}\right)^{-1}\left[D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}\right.\right.\nonumber\\
%  & \left.\left. \vphantom{\left(\mi+c^{-1}D_{t-1}\right)^{-1}}+c^{-1}\mi\right]\right)\vei{t-1}
%  +y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t}\nonumber\\
% & = \veti{t-1}
% \Bigg[\left(\mi+c^{-1}D_{t-1}\right)^{-1}D_{t}^{-1}\vxi{t}\vxti{t}D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}\nonumber\\
% &-D_{t-1}^{-1}
% +\left(\mi+c^{-1}D_{t-1}\right)^{-1}\left(D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}\right.\nonumber\\
% &\left.+c^{-1}\mi\right)\Bigg]\vei{t-1}+y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t} ~.\label{step2}
% \end{align}
Substituting the specific value of the predictor
$\hat{y}_{t}=\vxti{t}D_{t}^{-1}D'_{t-1}\vei{t-1}$
from \eqref{my_predictor}, we proceed to develop
the last equality,
% get that \eqref{step1} equals to,
% \begin{align}
%   & \hat{y}_{t}^{2}+y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t}+\veti{t-1}\Bigg[-D_{t-1}^{-1}\nonumber\\
% &+D'_{t-1}\left(D_{t}^{-1}D'_{t-1}+c^{-1}\mi\right)\Bigg]\vei{t-1}\nonumber\\
%  =& \veti{t-1}D'_{t-1}D_{t}^{-1}\vxi{t}\vxti{t}D_{t}^{-1}D'_{t-1}\vei{t-1}+\veti{t-1}\Bigg[-D_{t-1}^{-1}\nonumber\\
% &+D'_{t-1}\left(D_{t}^{-1}D'_{t-1}+c^{-1}\mi\right)\Bigg]\vei{t-1}
%  +y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t}\nonumber\\
%  =& \veti{t-1}
% \Bigg[D'_{t-1}D_{t}^{-1}\vxi{t}\vxti{t}D_{t}^{-1}D'_{t-1}-D_{t-1}^{-1}\nonumber\\
% &+D'_{t-1}\left(D_{t}^{-1}D'_{t-1}+c^{-1}\mi\right)\Bigg]\vei{t-1}+y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t} ~.\label{step2}
% \end{align}
\begin{eqnarray}
& =  &
  \hat{y}_{t}^{2}+y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t}+\veti{t-1}\Bigg[-D_{t-1}^{-1}+D'_{t-1}\left(D_{t}^{-1}D'_{t-1}+c^{-1}\mi\right)\Bigg]\vei{t-1}\nonumber\\
& = &
 \veti{t-1}D'_{t-1}D_{t}^{-1}\vxi{t}\vxti{t}D_{t}^{-1}D'_{t-1}\vei{t-1}+y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t}\nonumber\\
 &&+\veti{t-1}\Bigg[-D_{t-1}^{-1}+D'_{t-1}\left(D_{t}^{-1}D'_{t-1}+c^{-1}\mi\right)\Bigg]\vei{t-1}
 \nonumber\\
& = & \veti{t-1} \tilde{D}_t \vei{t-1}+y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t} ~,\label{step2}
\end{eqnarray}
 where $\tilde{D}_t =
 D'_{t-1}D_{t}^{-1}\vxi{t}\vxti{t}D_{t}^{-1}D'_{t-1}-D_{t-1}^{-1}+D'_{t-1}\left(D_{t}^{-1}D'_{t-1}+c^{-1}\mi\right)$.
Using \lemref{lem:technical} we upper bound $\tilde{D}_t
\preceq 0$ and thus \eqref{step2} is bounded,
\[
y_{t}^{2}\vxti{t}D_{t}^{-1}\vxi{t} \leq
Y^{2}\vxti{t}D_{t}^{-1}\vxi{t} ~.
\]
%
Finally, summing over $t\in\left\{ 1,\ldots,T\right\} $ gives the
desired bound,
\begin{align*}
% &\sum_{t=1}^{T}\left(y_{t}-\hat{y}_{t}\right)^{2}-\min_{\vui{1},\ldots,\vui{T}} \left[b\left\Vert
%     \vui{1}\right\Vert ^{2} +c\sum_{t=1}^{T-1}\left\Vert
%     \vui{t+1}-\vui{t}\right\Vert
%   ^{2}\right.\\
%   & \left. \quad+\sum_{t=1}^{T}\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\right]\\
&L_T(\texttt{LASER})-\min_{\vui{1},\ldots,\vui{T}}\left[b\left\Vert
    \vui{1}\right\Vert ^{2}
+c V_T(\{\vui{t}\})%\sum_{t=1}^{T-1}\left\Vert   \vui{t+1}-\vui{t}\right\Vert ^{2}
+ L_T(\{\vui{t}\})
%+
%\sum_{t=1}^{T}\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}
\right]
\leq&
Y^{2}\sum_{t=1}^{T}\vxti{t}D_{t}^{-1}\vxi{t} ~.
% \leq Y^{2}\sum_{t=1}^{T}\vxti{t}D_{t}^{-1}\vxi{t}
\end{align*}
\QED
\end{proof}

In the next lemma we further bound the right term of
\eqref{bound_1}. %thmref{thm:basic_bound}.
This type of bound is based on the
usage of the covariance-like matrix $D$.
\begin{lemma}
\label{lem:bound_1}
\begin{equation}
\sum_{t=1}^{T}\vxti{t}D_{t}^{-1}\vxi{t}\leq\ln\left|\frac{1}{b}D_{T}\right|
%+\sum_{t=1}^{T}\ln\left|\left(\mi+c^{-1}D_{t-1}\right)\right|
+c^{-1}\sum_{t=1}^{T} \tr\paren{D_{t-1}} ~.\label{covariance_bound}
\end{equation}
\end{lemma}
%\kc{add details}
\begin{proof}
%\kc{Edward, please add, we omitted some details due to lack of space}
%\edward{Added}
Let $B_{t}\doteq D_{t}-\vxi{t}\vxti{t}=\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}\succ0$.
\begin{align*}
\vxti{t}D_{t}^{-1}\vxi{t} & =  \tr\left(\vxti{t}D_{t}^{-1}\vxi{t}\right)=\tr\left(D_{t}^{-1}\vxi{t}\vxti{t}\right)\\
 & = \tr\left(D_{t}^{-1}\left(D_{t}-B_{t}\right)\right)\\
 & = \tr\left(D_{t}^{-1/2}\left(D_{t}-B_{t}\right)D_{t}^{-1/2}\right)\\
 & = \tr\left(\mi-D_{t}^{-1/2}B_{t}D_{t}^{-1/2}\right)\\
 & =
 \sum_{j=1}^{d}\left[1-\lambda_{j}\left(D_{t}^{-1/2}B_{t}D_{t}^{-1/2}\right)\right] ~.
\end{align*}
We continue using $1-x\leq-\ln\left(x\right)$ and get
\begin{align*}
\vxti{t}D_{t}^{-1}\vxi{t} & \leq  -\sum_{j=1}^{d}\ln\left[\lambda_{j}\left(D_{t}^{-1/2}B_{t}D_{t}^{-1/2}\right)\right]\\
 & = -\ln\left[\prod_{j=1}^{d}\lambda_{j}\left(D_{t}^{-1/2}B_{t}D_{t}^{-1/2}\right)\right]\\
 & = -\ln\left|D_{t}^{-1/2}B_{t}D_{t}^{-1/2}\right|\\
 & =
 \ln\frac{\left|D_{t}\right|}{\left|B_{t}\right|}=\ln\frac{\left|D_{t}\right|}{\left|D_{t}-\vxi{t}\vxti{t}\right|} ~.
\end{align*}
It follows that,
\begin{align*}
\vxti{t}D_{t}^{-1}\vxi{t}
& \leq
\ln\frac{\left|D_{t}\right|}{\left|\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}\right|}\\
 & =  \ln\frac{\left|D_{t}\right|}{\left|D_{t-1}\right|}\left|\left(\mi+c^{-1}D_{t-1}\right)\right|\\
 & =
\ln\frac{\left|D_{t}\right|}{\left|D_{t-1}\right|}+\ln\left|\left(\mi+c^{-1}D_{t-1}\right)\right| ~.
\end{align*}
 and because $\ln\left|\frac{1}{b}D_{0}\right| \geq 0$ we get
\begin{align*}
\sum_{t=1}^{T}\vxti{t}D_{t}^{-1}\vxi{t}&\leq
\ln\left|\frac{1}{b}D_{T}\right|
+\sum_{t=1}^{T}\ln\left|\left(\mi+c^{-1}D_{t-1}\right)\right|\\
& \leq \ln\left|\frac{1}{b}D_{T}\right|
 +c^{-1}\sum_{t=1}^{T} \tr\paren{D_{t-1}} ~.
\end{align*}
\QED
\end{proof}

At first sight it seems that the right term of
\eqref{covariance_bound} may grow super-linearly with $T$, as each of the
matrices $D_{t}$ grows with $t$. The next two lemmas show that this
is not the case, and in fact, the right term of
\eqref{covariance_bound} is not growing too fast, which will allow us to
obtain a sublinear regret bound. \lemref{operator_scalar} analyzes
the properties of the recursion of $D$ defined in
\eqref{D} for scalars, that is $d=1$. In \lemref{eigen_values_lemma} we extend
this analysis to matrices.
\begin{lemma}
\label{operator_scalar}
Define
\(
f(\lambda) = {\lambda \beta}/\paren{\lambda+ \beta} + x^2
\)
for $\beta,\lambda \geq 0$ and some $x^2 \leq \gamma^2$. Then:
\begin{enumerate}
\item $f(\lambda) \leq \beta +
\gamma^2$
\item $f(\lambda) \leq \lambda + \gamma^2$
\item $f(\lambda) \leq \max\braces{\lambda,\frac{3\gamma^2 +
  \sqrt{\gamma^4+4\gamma^2\beta}}{2}}$
\end{enumerate}
\end{lemma}
\begin{proof}
For the first property we have $f(\lambda) = {\lambda
  \beta}/\paren{\lambda+ \beta} + x^2 \leq \beta\times 1 + x^2$.
% We compute the derivative of $f(\lambda)$ with respect to $\lambda$
% and  get \(
% f'(\lambda) = \frac{\beta(\beta+\lambda) -
%   \beta\lambda}{(\beta+\lambda)^2} = \frac{\beta^2}{(\beta+\lambda)^2}
% \geq 0
% \) . Thus $f(\lambda)$ is increasing with $\lambda$ and thus we obtain
% the first property \(
% f(\lambda) \leq \lim_{z\rightarrow\infty} f(z) =
% \lim_{z\rightarrow\infty}  \frac{\beta}{1+ \beta/z} + x^2 =
% \beta+x^2\leq \beta+\gamma^2
% \).
The second property follows from the symmetry between $\beta$ and
$\lambda$.
%
To prove the third property we decompose the function as,
\(
f(\lambda) = \lambda - \frac{\lambda^2 }{\lambda+ \beta} + x^2
\).
Therefore, the function is bounded by its argument $f(\lambda)\leq
\lambda$ if, and only if, $- \frac{\lambda^2 }{\lambda+ \beta} + x^2
\leq 0$.
Since we assume $x^2\leq\gamma^2$, the last inequality holds if,
\(
-\lambda^2 + \gamma^2 \lambda + \gamma^2\beta \leq 0
%\quad\Leftrightarrow\quad\lambda^2 - \gamma^2 \lambda - \gamma^2\beta
%\geq 0 ~,
\),
which holds for $\lambda \geq \frac{\gamma^2 +
  \sqrt{\gamma^4+4\gamma^2\beta}}{2}$.

To conclude. If $\lambda \geq \frac{\gamma^2 +
  \sqrt{\gamma^4+4\gamma^2\beta}}{2}$, then $f(\lambda) \! \leq \! \lambda$.
Otherwise, by the second property, we have, $$f(\lambda) \! \leq \! \lambda\!+\!\gamma^2
\! \leq \! \frac{\gamma^2 \!+\!
  \sqrt{\gamma^4\!+\!4\gamma^2\beta}}{2} \!+\! \gamma^2 = \frac{3\gamma^2 \!+\!
  \sqrt{\gamma^4\!+\!4\gamma^2\beta}}{2},$$as required.%which concludes the proof.
\QED
\end{proof}

We build on \lemref{operator_scalar} to bound the maximal eigenvalue of the
matrices $D_{t}$.
\begin{lemma}
\label{eigen_values_lemma}
Assume $\normt{\vxi{t}} \leq X^2$ for some $X$. Then, the eigenvalues
of $D_{t}$ (for $t \geq 1$), denoted by $\lambda_i\paren{D_{t}}$, are upper bounded
by
\[\max_i\lambda_i\paren{D_{t}}\leq\max\braces{ \frac{3X^2 +
  \sqrt{X^4+4X^2 c}}{2},b+X^2} ~. \]
\end{lemma}
\begin{proof}
  By induction.  From \eqref{D} we have that
  $\lambda_i(D_{1}) \leq b + X^2$  for
$i=1\comdots d$. We proceed with a proof for some $t$. For simplicity,
denote by $\lambda_i = \lambda_i(D_{t-1})$ the i$th$
eigenvalue of $D_{t-1}$ with a corresponding
eigenvector $\vvi{i}$.
From \eqref{D} we have,
\begin{align}
D_{t}
&=\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}+\vxi{t}\vxti{t} \preceq \left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}  +
\mi \normt{\vxi{t}}\nonumber\\
& = \sum_i^d \vvi{i}
\vvti{i}\paren{\paren{\lambda_i^{-1} + c^{-1}}^{-1} + \normt{\vxi{t}}}= \sum_i^d \vvi{i}
\vvti{i}\paren{ \frac{\lambda_i c }{\lambda_i + c} +
  \normt{\vxi{t}}} ~.\label{bound_eigens}
\end{align}
Plugging  \lemref{operator_scalar} in \eqref{bound_eigens} we get,
\begin{align*}
D_{t}
&\preceq \sum_i^d \vvi{i}
\vvti{i}\max\braces{ \frac{3X^2 +
  \sqrt{X^4+4X^2 c}}{2},b+X^2}\\
% = \max\braces{ \frac{3X^2 +
%   \sqrt{X^4+4X^2 c}}{2},b+X^2} \sum_i^d \vvi{i}\vvti{i}\\
% &
&= \max\braces{ \frac{3X^2 +
  \sqrt{X^4+4X^2 c}}{2},b+X^2} \mi~,
\end{align*}
as required.
\QED
\end{proof}

Finally, equipped with the above lemmas we are able to
prove the main
result of this section.
\begin{corollary}
\label{cor:main_corollary}
Assume $\normt{\vxi{t}}\leq X^2$, $\vert\yi{t}\vert \leq Y$. Then,
\begin{eqnarray}
L_T(\texttt{LASER})&\leq&
  b\left\Vert
     \mathbf{u}_{1}\right\Vert ^{2}+L_T(\{\vui{t}\})+Y^{2} \ln\left|\frac{1}{b}\mathbf{D}_{T}\right|
     +c^{-1}Y^2\tr\paren{\mathbf{D}_0}+c V\nonumber\\
& & +c^{-1}Y^2 T d  \max\braces{ \frac{3X^2 +
   \sqrt{X^4+4X^2 c}}{2},b+X^2}~.\label{final_cor}
\end{eqnarray}

Furthermore, set $b=\varepsilon c$ for some $0<\varepsilon<1$.
Denote by
\(
\mu = \max\braces{9/8X^2, \frac{\paren{b+X^2}^2}{8X^2}}
\).
% and %\quad,\quad
%\(M =
%\max\braces{3X^2, b+X^2}
%\)
%\begin{enumerate}
%\item{\bf Low Drift: }
If $V \leq T \frac{\sqrt{2}Y^2dX}{\mu^{3/2}}$ then by setting
\begin{align}
c= \frac{\sqrt{2}T Y^2 d X}{V^{2/3}}\label{c1}
\end{align}
 we have,
\begin{eqnarray}
L_T(\texttt{LASER})
&\leq& b\left\Vert \vui{1}\right\Vert ^{2} + 3\paren{\sqrt{2}Y^2 d X}^{2/3} T^{2/3} V^{1/3}+\frac{\varepsilon}{1-\varepsilon}Y^{2}d +L_T(\{\vui{t}\})%\sum_{t=1}^{T}\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}
\nonumber\\
&& +~Y^{2} \ln\left|\frac{1}{b}D_{T}\right|\label{bound1}~.
\end{eqnarray}

%\item{\bf High Drift: }
% If $V^{(2)} \geq T \frac{Y^2dM}{\mu^{2}}$ then by setting
%\begin{align}
%c=
% \sqrt{\frac{Y^2 dMT}{V^{(2)}}}\label{c2}
%\end{align}
% we have,
%\begin{align*}
%L_T(\textrm{LASER})
%&\leq b\left\Vert \vui{1}\right\Vert ^{2}
% + 2\sqrt{Y^2 d TMV^{(2)}}\\
%& +\frac{\varepsilon}{1-\varepsilon}Y^{2}d +L_T(\{\vui{t}\})%\sum_{t=1}^{T}\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}
%+Y^{2} \ln\left|\frac{1}{b}D_{T}\right|~.
%\end{align*}
%\end{enumerate}

% \begin{enumerate}
% \item{\bf Low Drift: }
% If $V^{(2)} \leq T \frac{\sqrt{2}Y^2dX}{\mu^{3/2}}$ then by setting
% \begin{align}
% c= \paren{\frac{\sqrt{2}T Y^2 d X}{V^{(2)}}}^{2/3}\label{c1}
% \end{align}
%  we have,
% \begin{align*}
% & L_T(\textrm{LASER})
% \leq\\
% & \quad b\left\Vert \vui{1}\right\Vert ^{2} + 3\paren{\sqrt{2}Y^2 d X}^{2/3} T^{2/3} \paren{V^{(2)}}^{1/3}\\
% &\quad +\frac{\varepsilon}{1-\varepsilon}Y^{2}d +L_T(\{\vui{t}\})%\sum_{t=1}^{T}\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}
% +Y^{2} \ln\left|\frac{1}{b}D_{T}\right|.%\label{bound1}
% \end{align*}

% \item{\bf High Drift: }
%  If $V^{(2)} \geq T \frac{Y^2dM}{\mu^{2}}$ then by setting
% \begin{align}
% c=
%  \sqrt{\frac{Y^2dMT}{V^{(2)}}}\label{c2}
% \end{align}
%  we have,
% \begin{align*}
% & L_T(\textrm{LASER})
% \leq b\left\Vert \vui{1}\right\Vert ^{2}
%  + 2\sqrt{Y^2 d TMV^{(2)}}\\
% & \quad+\frac{\varepsilon}{1-\varepsilon}Y^{2}d +L_T(\{\vui{t}\})%\sum_{t=1}^{T}\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}
% +Y^{2} \ln\left|\frac{1}{b}D_{T}\right|.
% \end{align*}
% \end{enumerate}
\end{corollary}
%The proof appears in \secref{proof_cor_main}.
\begin{proof}
Plugging \lemref{lem:bound_1} in \thmref{thm:basic_bound} we have for
all $(\vui{1} \comdots \vui{T})$,
\begin{align}
L_T(\texttt{LASER})
%\sum_{t=1}^{T}\left(y_{t}-\hat{y}_{t}\right)^{2}
 \leq~&  b\left\Vert
    \vui{1}\right\Vert ^{2}+c V+L_T(\{\vui{t}\})+Y^{2} \ln\left|\frac{1}{b}D_{T}\right|+c^{-1}Y^2 \sum_{t=1}^{T} \tr\paren{D_{t-1}} \nonumber\\
\leq~ &
  b\left\Vert
     \vui{1}\right\Vert ^{2}+L_T(\{\vui{t}\})+Y^{2}
   \ln\left|\frac{1}{b}D_{T}\right|+c^{-1}Y^2\tr\paren{D_0}+c V \nonumber \\
& +c^{-1}Y^2 T d  \max\braces{ \frac{3X^2 +
   \sqrt{X^4+4X^2 c}}{2},b+X^2}~,\nonumber%\label{final_cor}
\end{align}
where the last inequality follows from \lemref{eigen_values_lemma}.
%
The term $c^{-1}Y^2\tr\paren{D_0}$ does not depend on $T$, because
\[
c^{-1}Y^2\tr\paren{D_0}=c^{-1}Y^2d\frac{bc}{c-b}=\frac{\varepsilon}{1-\varepsilon}Y^{2}d~.
\]
Next, note that
\[
V \leq T \frac{\sqrt{2}Y^2dX}{\mu^{3/2}} \Leftrightarrow \mu \leq  \paren{\frac{\sqrt{2}Y^2dXT}{V}}^{2/3}=c~.
\]
We thus have that the right term of \eqref{final_cor} is upper bounded,
\begin{align*}
\max\braces{  \frac{ 3X^2 +
   \sqrt{ X^4+4X^2 c } }{ 2 },b+X^2 }
& \leq~  \max\braces{  \frac{ 3X^2 +
   \sqrt{ 8X^2 c } }{ 2 },b+X^2 }\\
&    \leq~
\max\braces{  \sqrt{ 8X^2 c },b+X^2 }
\leq 2X\sqrt{ 2c } ~.
\end{align*}
Using this bound and plugging the value of $c$ from \eqref{c1} we bound
\eqref{final_cor},
\begin{align*}
&\paren{\frac{\sqrt{2}T Y^2 d X}{V}}^{2/3} V + Y^2 T d 2X
\sqrt{2 \paren{\frac{\sqrt{2}T Y^2 d X}{V}}^{-2/3}}\\
& \quad=
3\paren{\sqrt{2}T Y^2 d X}^{2/3} V^{1/3} ~,
\end{align*}
which concludes the proof.
\QED
\end{proof}

\section{Comparison to other bounds}

In this section we compare bounds of non-stationary regression algorithms. First, note that the regret bound of \texttt{LASER} in \eqref{bound1} is of the order of $T^{2/3} V^{1/3}$ and thus the algorithm maintains an average loss close to that of the best slowly changing sequence of linear functions, as long as the total of drift $V$ is sublinear.
The regret of the ARCOR algorithm~\citep{VaitsCr11} depends on the total drift as $\sqrt{T V'}\log(T)$, where their definition of total drift is a sum of the Euclidean differences $V'=\sum_t^{T-1} \Vert \vui{t+1}-\vui{t}\Vert$, rather than the squared norm.
The two bounds are not comparable in general. However, we can compare the bounds on two concrete analytical examples:
\begin{enumerate}
\item Assume a
constant instantaneous drift $\Vert\vui{t+1}-\vui{t}\Vert=\nu$ for
some constant value $\nu$. In this case the variance and squared variance are,
$V'=T\nu$ and $V=T\nu^2$. The bound of ARCOR becomes
$ \nu^{\half}T\log T$, while the bound of \texttt{LASER} becomes $\nu^\frac{2}{3}T$.
The bound of ARCOR is larger if $(\log T)^6 > \nu$, and the bound of
\texttt{LASER} is larger in the opposite case.
\item Assume a polynomial decay of the drift,
$\Vert\vui{t+1}-\vui{t}\Vert \leq t^{-\kappa}$ for some $\kappa > 0$.
In this case, for $\kappa \neq 1$ we get $V' \leq \sum^{T-1}_{t=1} t^{-\kappa} \leq \int^{T-1}_1 t^{-\kappa}  dt+1=\frac{(T-1)^{1-\kappa}-\kappa}{1-\kappa}$. For $\kappa=1$ we get $V' \leq \log(T-1)+1$.
For \texttt{LASER} we have, for $\kappa \neq 0.5$, $V \leq \sum^{T-1}_{t=1} t^{-2\kappa} \leq \int^{T-1}_1 t^{-2\kappa}  dt+1=\frac{(T-1)^{1-2\kappa}-2\kappa}{1-2\kappa}$. For $\kappa=0.5$ we get $V \leq \log(T-1)+1$.
Asymptotically, \texttt{LASER} outperforms ARCOR about when $\kappa < 0.7$.
\end{enumerate}

\cite{HerbsterW01} developed shifting bounds for general
gradient descent algorithms with projection of the weight-vector using
the Bregman divergence. In their bounds, there is a factor greater
than 1 multiplying the term $L_T\paren{\braces{\vui{t}}}$, leading to a
small regret only when the data is close to be realizable with
linear models.~\cite{BusuttilK07} developed a variant of the Aggregating Algorithm~\citep{vovkAS} for the non-stationary setting. However, to have sublinear regret they require a strong
assumption on the drift $V=o(1)$, while we require only
$V=o(T)$.

