\chapter{Introduction}

We consider the online learning regression problem, in which a learning algorithm tries to predict real numbers in a sequence of rounds given some side-information or inputs $\vxi{t}\in\reals^d$. Real-world example applications for these algorithms are weather or stock market predictions. The goal of an algorithm is to have a small discrepancy between its predictions and the associated outcomes or labels  $\yi{t}\in\reals$, which can be chosen by an adversary. This discrepancy is measured with a loss function, such as the square loss. It is common to evaluate algorithms by their regret, the difference between the cumulative loss of an algorithm with the cumulative loss of any function taken from some class.

Many algorithms were proposed for this problem. Gradient descent based algorithms were analysed by~\cite{Nicolo_Warmuth}, and it was shown that a regret of $\mcal{O}(\sqrt{T})$ can be achieved where $T$ is the total number of rounds.~\cite{Kiv_War} proposed the Exponentiated Gradient Algorithm that uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. This algorithm achieves also a regret of $\mcal{O}(\sqrt{T})$, but has a much smaller loss if only few components of the input are relevant for the predictions. Similar regret was shown in the online convex programming framework~\citep{Zinkevich03onlineconvex}.

In this work we focus on second order algorithms. These algorithms maintain a weight vector used for prediction and a covariance-like matrix which functions like adaptive learning rate. The RLS algorithm~\citep{Hayes}, Ridge-Regression~\citep{Foster91} and the AAR algorithm~\citep{Vovk97,Vovk01} are examples of second order algorithms, all achieve a regret of $\mcal{O}(\log{T})$, yet with different multiplicative factors of the logarithm. Another similar algorithm with similar regret (AROWR) proposed by~\cite{VaitsCr11} and analysed by~\cite{CrammerKuDr12}. Recently, ~\cite{OrabonaCBG12} showed
that beyond logarithmic regret bound can be achieved when the total
best linear model loss is sublinear in $T$.

~\cite{Forster} proposed a last-step min-max algorithm for online regression that makes a prediction assuming it is the last example to be observed, and the goal of the algorithm is indeed to minimize the maximal (worst-case) regret with respect to linear functions.~\cite{TakimotoW00} used this idea for the online density estimation problem. The resulting optimization problem of~\cite{Forster} was convex in both choice of the algorithm and the choice of the adversary, yielding an unbounded optimization problem.~\cite{Forster} circumvented this problem by assuming a bound $Y$ over the choices of the adversary that should be known to the algorithm, yet his analysis is for the version with no bound, leading to a regret of $\mcal{O}(\log{T})$.

We propose a modified last-step min-max algorithm with weights over examples, that are controlled in a way to obtain a problem that is concave over the choices of the adversary and convex over the choices of the algorithm. We analyze our algorithm and show a logarithmic-regret that may have a better multiplicative factor than the analysis of~\cite{Forster} or other algorithms. We derive additional analysis that is logarithmic in the loss of the reference function, rather than the number of rounds $T$. This behaviour was recently given by~\cite{OrabonaCBG12} for the Online Newton Step algorithm~\citep{Hazan06logarithmicregret}. Yet, their bound~\citep{OrabonaCBG12} has a similar multiplicative factor to that of
~\cite{Forster}, while our bound has a potentially better multiplicative factor and it has the same dependency in the cumulative loss of the reference function as~\cite{OrabonaCBG12}. Additionally, our algorithm and analysis are totally free of assuming the bound $Y$ or knowing its value.

Competing with the best {\em fixed} function might not suffice for
some problems. In many real-world applications, the true target
function is not fixed, but is slowly drifting over time. This setting is called non-stationary setting. Consider a
function designed to rate movies for recommender systems given some
features. Over time a rate of a movie may change as more movies are
released or the season changes. Furthermore, the very own
personal-taste of a user may change as well.

These reasons led to the development of algorithms and accompanying analysis
for drifting and shifting settings (see for example
\cite{ECCC-TR00-070,HerbsterW01,KivinenSW01,CavallantiCG07} and the references therein).
The goal of an algorithm in this settings is to
maintain an average loss close to that of the best slowly changing
sequence of functions, rather than compete well with a single
function. We focus on problems for which this sequence consists only
of linear functions. Often such a sequence is either drifting, where each
function is close in some sense to its predecessor, or shifting, where
conceptually the sequence can be partitioned into few segments, for each
there is a single function that performs well on all examples of that
segment.

One approach used in previous algorithms for non-stationary
setting is based on gradient descent with additional control on the norm of the weight-vector used for prediction.
Bounding the weight vector was performed either by projecting it into a bonded
set~\citep{HerbsterW01}, shrinking it by
multiplication~\citep{KivinenSW01,CavallantiCG07} or subtraction of previously seen
examples~\citep{CavallantiCG07}, which was motivated from reduction of memory when using kernels~\citep{CrammerKS03}. Since the tracking ability is naturally connected to a weakened dependence on the past, memory boundedness could be viewed as a way to obtain a good performance in non-stationary setting. Another approach, which is used in second order algorithms, is based on resetting the covariance matrix, and in this way forgetting the history. The Covariance Reset RLS algorithm
(CR-RLS)~\citep{Salgado,Goodwin,Chen} was designed for adaptive
filtering and it makes covariance reset every fixed amount of rounds. The ARCOR algorithm~\citep{VaitsCr11} is similar to the CR-RLS algorithm but it resets the covariance matrix each time the smallest eigenvalue of the covariance matrix reaches a certain threshold. Thus, the ARCOR algorithm performs resets based on the actual properties of the data: the eigenspectrum of the covariance matrix. In addition, the ARCOR algorithm projects the weight vector into a bounded set in order to prevent it from going too far in some directions.

We take a different route and derive an algorithm for the non-stationary setting based on the
last-step min-max approach of~\cite{Forster}. Yet, our algorithm is min-max optimal when a drift is
allowed. As opposed to the derivation of the last-step min-max
predictor for a fixed competitor, the resulting optimization
problem is not straightforward to solve. We develop a dynamic program (a
recursion) to solve this problem, which allows to compute the optimal
last-step min-max predictor. We analyze the algorithm in the
worst-case regret framework and show that the algorithm maintains an
average loss close to that of the best slowly changing sequence of
functions, as long as the total drift is sublinear in the number of
rounds $T$. Additionally, when no drift is
introduced (stationary setting) our algorithm suffers logarithmic
regret, as for the algorithm of~\cite{Forster}.

%Additionally, we build on the $H_\infty$ adaptive filter, which is min-max optimal
%with respect to a {\em filtering task}, and derive another learning
%algorithm based on the same min-max principle.
% We provide a regret bound for this algorithm as well, and compare it to the last-step min-max algorithm that based on the approach of~\cite{Forster}.

Finally, synthetic simulations show the
advantages of our algorithm. % in a worst-case constant drift setting.
