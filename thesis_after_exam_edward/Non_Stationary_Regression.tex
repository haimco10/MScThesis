\chapter{Non-Stationary Regression}

Consider the problem of prediction of life expectancy in some country.
As the medicine advances and anti-aging techniques develop, there is a drift in
the correct model that should be used to describe life expectancy.
This is an example of prediction in non-stationary environment. Learning in such
environments requires prediction algorithms to have the ability to track changes in the data.

For the stationary algorithms in the previous chapters (see \tabref{table:algorithms}), the update rule of
$\mathbf{\Sigma}_{t}^{-1}$ forces the eigenvalues of $\mathbf{\Sigma}_{t}^{-1}$ to increase, and thus the eigenvalues of $\mathbf{\Sigma}_{t}$ go to zero with time.
Thus, the effective learning rate goes to zero.
As a consequence, these algorithms will gradually stop updating using instances
which lie in the subspace of examples that were previously observed numerous
times. This property leads to fast convergence in the stationary case, but at the
same time to poor performance in the non-stationary case. It might happen that
there is a need to update the prediction rule using an instance, yet the learning rate
for this specific update is too small, and no useful update can be performed.

In this chapter we extend the last-step min-max rule of~\cite{Forster} to the non-stationary setting.
In \secref{sec:LASER_alg} we formally define the min-max problem for non-stationary setting, and
our prediction algorithm will be the solution of this problem.
In \secref{sec:LASER_recursive} we show that our algorithm can be expressed in a recursive form,
similar to other algorithms. Finally, in \secref{sec:LASER_comparison} we compare our algorithm to other algorithms of the same form.
The analysis of the algorithm appears in \chapref{chap:LASER_analysis},
where we derive a regret bound for the algorithm, and compare it to other bounds.


\section{LASER algorithm}
\label{sec:LASER_alg}

%In this section we derive an algorithm for non-stationary setting. Our algorithm is based on a last-step min-max approach
%proposed by~\cite{Forster}. On each round, the algorithm predicts as it is the
%last round, and assumes a worst case choice of $\yi{t}$ given the
%algorithm's prediction.

We extend the rule given in \eqref{forster_minmax} to the non-stationary setting and re-define the last-step min-max
predictor $\hyi{T}$ to be,
%\footnote{$\yi{T}$ and $\hyi{T}$ serve both
%  as quantifiers (over the $\min$ and $\max$ operators, respectively),
%  and as the optimal arguments of this optimization problem. }
%To develop the last step optimal predictor we calculate:
\begin{align}
\arg\min_{\hat{y}_{T}}\max_{y_{T}}\left[\sum_{t=1}^{T}\left(y_{t}-\hat{y}_{t}\right)^{2}\!-\!\!\!\min_{\vui{1},
    .. ,\vui{T}}
  \!\! Q_{T}\left(\vui{1}, ... ,\vui{T}\right)\right],\label{minmax_1}
\end{align}
where,
\begin{align}
Q_{t}\left(\vui{1},\ldots,\vui{t}\right) = b\left\Vert
  \vui{1}\right\Vert ^{2}+c\sum_{s=1}^{t-1}\left\Vert
  \vui{s+1}-\vui{s}\right\Vert ^{2}
+\sum_{s=1}^{t}\left(y_{s}-\vuti{s}\vxi{s}\right)^{2}~,\label{Q}
\end{align}
for some positive constants $b,c$. The first term of \eqref{minmax_1}
is the loss suffered by the algorithm while
$Q_{t}\left(\vui{1},\ldots,\vui{t}\right)$ defined in
\eqref{Q} is a sum of the loss suffered by some sequence of linear
functions $\left(\vui{1},\ldots,\vui{t}\right)$ and a
penalty for consecutive pairs that are far from each other, and for the
norm of the first to be far from zero.

%
We develop the algorithm by solving the three optimization problems in
\eqref{minmax_1}, first, minimizing the inner term,
$\min_{\vui{1}, .. ,\vui{T}}
Q_{T}\left(\vui{1}, ... ,\vui{T}\right)$, second maximizing
over $\vyi{T}$, and finally, minimizing over $\hyi{T}$.  We start with
the inner term
% first solve
% recursively the inner optimization problem
% $\min_{\vui{1},\ldots,\vui{t}}Q_{t}\left(\vui{1},\ldots,\vui{t}\right)$,
for which we define an auxiliary function,
\begin{align*}
P_{t}\left(\vui{t}\right)=\min_{\vui{1},\ldots,\vui{t-1}}Q_{t}\left(\vui{1},\ldots,\vui{t}\right) ~,%\label{P}
\end{align*}
 which clearly satisfies,
\begin{equation*}
\min_{\vui{1},\ldots,\vui{t}}Q_{t}\left(\vui{1},\ldots,\vui{t}\right)
= \min_{\vui{t}} P_t(\vui{t}) ~.%\label{PQ}
\end{equation*}

The
%We start the derivation of the algorithm with the
following lemma states a recursive form of the function-sequence $P_t(\vui{t})$.
\begin{lemma}
\label{lem:lemma11}
For $t=2,3,\ldots$
% \begin{equation*}
% \begin{split}
%  P_1(\vui1)=Q_1(\vui1)\quad,\quad
%  P_{t}&\left(\vui{t}\right)=
% \min_{\vui{t-1}}\left(P_{t-1}\left(\vui{t-1}\right)\phantom{\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}}\right.\\
%   &\left. +c\left\Vert
%     \vui{t}-\vui{t-1}\right\Vert
%   ^{2} +\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\right).
% \end{split}
% \end{equation*}
%
\begin{align*}
P_1(\vui1)&=Q_1(\vui1)\\
 P_{t}\left(\vui{t}\right)&=
\min_{\vui{t-1}}\left(P_{t-1}\left(\vui{t-1}\right)
\!\!+\!\!c\left\Vert
    \vui{t}-\vui{t-1}\right\Vert
  ^{2} \!\!+\!\!\left(y_{t}\!-\!\vuti{t}\vxi{t}\right)^{2}\right).
\end{align*}
 \end{lemma}
%The proof appears in \secref{proof_lemma11}.
\begin{proof}
We calculate
\begin{align*}
P_{t}\left(\vui{t}\right)
=&  \min_{\vui{1},\ldots,\vui{t-1}} \Bigg(b\left\Vert \vui{1}\right\Vert ^{2}+c\sum_{s=1}^{t-1}\left\Vert \vui{s+1}-\vui{s}\right\Vert ^{2}+\sum_{s=1}^{t}\left(y_{s}-\vuti{s}\vxi{s}\right)^{2}\Bigg)\\
 =&   \min_{\vui{1},\ldots,\vui{t-1}}
\Bigg(b\left\Vert \vui{1}\right\Vert
^{2}+c\sum_{s=1}^{t-2}\left\Vert
  \vui{s+1}-\vui{s}\right\Vert
^{2}+\sum_{s=1}^{t-1}\left(y_{s}-\vuti{s}\vxi{s}\right)^{2}\\
&+c\left\Vert \vui{t}-\vui{t-1}\right\Vert ^{2}
+\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\Bigg)\\
  =&
  \min_{\vui{t-1}}\min_{\vui{1},\ldots,\vui{t-2}}\Bigg(b\left\Vert
    \vui{1}\right\Vert ^{2}+c\sum_{s=1}^{t-2}\left\Vert
    \vui{s+1}-\vui{s}\right\Vert
  ^{2}+\sum_{s=1}^{t-1}\left(y_{s}-\vuti{s}\vxi{s}\right)^{2}\\
&+c\left\Vert \vui{t}-\vui{t-1}\right\Vert ^{2}
+\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\Bigg)\\
  =&
 \min_{\vui{t-1}}\Bigg[\min_{\vui{1},\ldots,\vui{t-2}}
 \bigg(b\left\Vert
     \vui{1}\right\Vert ^{2}+c\sum_{s=1}^{t-2}\left\Vert
     \vui{s+1}-\vui{s}\right\Vert
   ^{2}+\sum_{s=1}^{t-1}\left(y_{s}-\vuti{s}\vxi{s}\right)^{2}\bigg)\\
&+c\left\Vert \vui{t}-\vui{t-1}\right\Vert ^{2}+\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\Bigg]\\
   =&  \min_{\vui{t-1}}\Bigg(P_{t-1}\left(\vui{t-1}\right)+c\left\Vert \vui{t}-\vui{t-1}\right\Vert ^{2}+\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\Bigg) ~.
\end{align*}
% \begin{align*}
% &P_{t}\left(\vui{t}\right)\\
% & =  \min_{\vui{1},\ldots,\vui{t-1}}  \left(b\left\Vert \vui{1}\right\Vert ^{2}+c\sum_{s=1}^{t-1}\left\Vert \vui{s+1}-\vui{s}\right\Vert^{2}+\sum_{s=1}^{t}\left(y_{s}-\vuti{s}\vxi{s}\right)^{2}\right)\\
%  & =  \min_{\vui{1},\ldots,\vui{t-1}}
% \Bigg(b\left\Vert \vui{1}\right\Vert
% ^{2}+c\sum_{s=1}^{t-2}\left\Vert
%   \vui{s+1}-\vui{s}\right\Vert
% ^{2}+\sum_{s=1}^{t-1}\left(y_{s}-\vuti{s}\vxi{s}\right)^{2}\\
% & \quad+c\left\Vert \vui{t}-\vui{t-1}\right\Vert ^{2}+\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\Bigg)\\
%   & =
%   \min_{\vui{t-1}}\min_{\vui{1},\ldots,\vui{t-2}}\Bigg(b\left\Vert
%     \vui{1}\right\Vert ^{2}+c\sum_{s=1}^{t-2}\left\Vert
%     \vui{s+1}-\vui{s}\right\Vert
%   ^{2}\\
% &\quad+\sum_{s=1}^{t-1}\left(y_{s}-\vuti{s}\vxi{s}\right)^{2}
% +c\left\Vert \vui{t}-\vui{t-1}\right\Vert ^{2}+\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\Bigg)\\
%  & =
%  \min_{\vui{t-1}}\Bigg[\min_{\vui{1},\ldots,\vui{t-2}}\left(b\left\Vert
%      \vui{1}\right\Vert ^{2}+c\sum_{s=1}^{t-2}\left\Vert
%      \vui{s+1}-\vui{s}\right\Vert
%    ^{2}\right.\\
%    &\quad \left.+\sum_{s=1}^{t-1}\left(y_{s}-\vuti{s}\vxi{s}\right)^{2}\right)+c\left\Vert \vui{t}-\vui{t-1}\right\Vert ^{2}+\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\Bigg]\\
%   & =  \min_{\vui{t-1}}\Bigg(P_{t-1}\left(\vui{t-1}\right)+c\left\Vert \vui{t}-\vui{t-1}\right\Vert ^{2}+\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\Bigg)
% \end{align*}
\QED
\end{proof}
Using \lemref{lem:lemma11} we write explicitly the function $P_t(\vui{t})$.
\begin{lemma}
\label{lem:lemma12}
The following equality holds
\begin{align*}
P_{t}\left(\vui{t}\right)=\vuti{t}D_{t}\vui{t}-2\vuti{t}\vei{t}+f_{t}~,
%\label{eqality_P}
\end{align*}
where,
\begin{align}
&D_{1} \!=b\mi+\vxi{1}\vxti{1}
~,~~
D_{t}=\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}+\vxi{t}\vxti{t}\label{D}\\
&\vei{1}\!=y_{1}\vxi{1}~,~~
\vei{t}=\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}+y_{t}\vxi{t}\label{e}\\
&f_{1}\!=y_{1}^{2}~,~~
f_{t}=f_{t-1}-\veti{t-1}\left(c\mi+D_{t-1}\right)^{-1}\vei{t-1}+y_{t}^{2}\label{f}
\end{align}
\end{lemma}
Note that $D_{t}\in\mathbb{R}^{d\times d}$ is a positive definite matrix,
$\vei{t}\in\mathbb{R}^{d\times1}$ and $f_{t}\in\mathbb{R}$.
%The proof appears in \secref{proof_lemma12}.
\begin{proof}
By definition,
\begin{align*}
P_{1}\left(\vui{1}\right) & = Q_{1}\left(\vui{1}\right)
 = b\left\Vert \vui{1}\right\Vert ^{2}+\left(y_{1}-\vuti{1}\vxi{1}\right)^{2}
 =\vuti{1}\left(b\mi+\vxi{1}\vxti{1}\right)\vui{1}-2y_{1}\vuti{1}\vxi{1}+y_{1}^{2} ~,
\end{align*}
and indeed
\(
D_{1}=b\mi+\vxi{1}\vxti{1}
\),
\(
\vei{1}=y_{1}\vxi{1}
\), and
\(
f_{1}=y_{1}^{2}
\).

We proceed by induction, assume that,
\(
P_{t-1}\left(\vui{t-1}\right)=\vuti{t-1}D_{t-1}\vui{t-1}-2\vuti{t-1}\vei{t-1}+f_{t-1}
\).
Applying \lemref{lem:lemma11} we get,
\begin{align*}
P_{t}\left(\vui{t}\right) &= \min_{\vui{t-1}}\Bigg(\vuti{t-1}D_{t-1}\vui{t-1}-2\vuti{t-1}\vei{t-1}+f_{t-1}+c\left\Vert \vui{t}-\vui{t-1}\right\Vert ^{2}+\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\Bigg)\\
&  = \min_{\vui{t-1}}\Bigg(\vuti{t-1}\left(c\mi+D_{t-1}\right)\vui{t-1}-2\vuti{t-1}
\left(c\vui{t}+\vei{t-1}\right) +f_{t-1}+c\left\Vert \vui{t}\right\Vert ^{2}+\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\Bigg)\\
&  =  -\left(c\vui{t}+\vei{t-1}\right)^{\top}\left(c\mi+D_{t-1}\right)^{-1}\left(c\vui{t}+\vei{t-1}\right)
+f_{t-1}+c\left\Vert \vui{t}\right\Vert ^{2}+\left(y_{t}-\vuti{t}\vxi{t}\right)^{2}\\
&  = \vuti{t}\left(c\mi+\vxi{t}\vxti{t}-c^{2}\left(c\mi+D_{t-1}\right)^{-1}\right)\vui{t}-2\vuti{t}\left[c\left(c\mi+D_{t-1}\right)^{-1}\vei{t-1}+y_{t}\vxi{t}\right]\\
& \quad -\veti{t-1}
 \left(c\mi+D_{t-1}\right)^{-1}\vei{t-1}+f_{t-1}+y_{t}^{2}~.
\end{align*}
Using the Woodbury identity we continue to develop the last equation,
\begin{align*}
& = \vuti{t}\left(c\mi+\vxi{t}\vxti{t}-c^{2}\left[c^{-1}\mi-c^{-2}\left(D_{t-1}^{-1}+c^{-1}
\mi\right)^{-1}\right]\right)\vui{t}\\
 & \quad  -2\vuti{t}\left[\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}+y_{t}\vxi{t}\right]\\
 & \quad -\veti{t-1}
 \left(c\mi+D_{t-1}\right)^{-1}\vei{t-1}+f_{t-1}+y_{t}^{2}\\
 & =  \vuti{t}\left(\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}+\vxi{t}\vxti{t}\right)\vui{t}\\
 & \quad -2\vuti{t}\left[\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}+y_{t}\vxi{t}\right]\\
 & \quad -\veti{t-1}
 \left(c\mi+D_{t-1}\right)^{-1}\vei{t-1}+f_{t-1}+y_{t}^{2}~,
\end{align*}
and indeed
$
D_{t}=\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}+\vxi{t}\vxti{t}
$,
$
\vei{t}=\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}+y_{t}\vxi{t}
$ and,
$
f_{t}=f_{t-1}-\veti{t-1}\left(c\mi+D_{t-1}\right)^{-1}\vei{t-1}+y_{t}^{2}
$, as desired.
\QED
\end{proof}

From \lemref{lem:lemma12} we
conclude that,
\begin{align}
& \min_{\vui{1},\ldots,\vui{t}}Q_{t}\left(\vui{1},\ldots,\vui{t}\right)
=  \min_{\vui{t}}P_{t}\left(\vui{t}\right)\nonumber\\
& = \min_{\vui{t}}\left(\vuti{t}D_{t}\vui{t}-2\vuti{t}\vei{t}+f_{t}\right)
= -\veti{t}D_{t}^{-1}\vei{t}+f_{t}
 ~. \label{optimal_Q}
\end{align}
Substituting \eqref{optimal_Q} back in \eqref{minmax_1} we get that the last-step min-max
predictor is given by,
\begin{eqnarray}
\hyi{T}=\arg\min_{\hat{y}_{T}}\max_{y_{T}}\left[\sum_{t=1}^{T}\left(y_{t}-\hat{y}_{t}\right)^{2}+\veti{T}D_{T}^{-1}\vei{T}-f_{T}\right]
~. \label{minmax_2}
\end{eqnarray}
Since $\vei{T}$ depends on
$\yi{T} $ we substitute \eqref{e} in the second term
of \eqref{minmax_2}, % and get,
\begin{align}
\veti{T}D_{T}^{-1}\vei{T}=\left(\left(\mi+c^{-1}D_{T-1}\right)^{-1}\vei{T-1}+y_{T}\vxi{T}\right)^{\top}D_{T}^{-1}\left(\left(\mi+c^{-1}D_{T-1}\right)^{-1}\vei{T-1}+y_{T}\vxi{T}\right) ~.\label{second_term}
\end{align}
Substituting \eqref{second_term} and \eqref{f} in \eqref{minmax_2} and
omitting terms not depending explicitly on $y_{T}$ and $\hat{y}_{T}$
we get,
\begin{align}
\hat{y}_T
&= \arg\min_{\hat{y}_{T}}\max_{y_{T}}\bigg[\left(y_{T}-\hat{y}_{T}\right)^{2}  + y_{T}^{2}\vxti{T}D_{T}^{-1}\vxi{T}+2y_{T}\vxti{T}D_{T}^{-1}\left(\mi+c^{-1}D_{T-1}\right)^{-1}\vei{T-1}-y_{T}^{2}\bigg]\nonumber\\
&=\arg\min_{\hat{y}_{T}}\max_{y_{T}}
\bigg[\left(\vxti{T}D_{T}^{-1}\vxi{T}\right)y_{T}^{2}+2y_{T}\left(\vxti{T}D_{T}^{-1}\left(\mi+c^{-1}D_{T-1}\right)^{-1}\vei{T-1}-\hat{y}_{T}\right)+\hat{y}_{T}^{2}\bigg]~.
\label{optimal_y}
\end{align}
The last equation is strictly convex in $\yi{T}$ and thus the optimal
solution is not bounded. To solve it, we follow an approach used by~\cite{Forster}. In order to make the optimal
value bounded, we assume that the adversary can only
choose labels from a bounded set $\yi{T}\in[-Y,Y]$. Thus, the optimal
solution of \eqref{optimal_y} over $\yi{T}$ is given by the following
equation, since the optimal value is $\yi{T}\in\{+Y,-Y\}$,
\begin{align*}
\hat{y}_T
=\arg\min_{\hat{y}_{T}}\bigg[\left(\vxti{T}D_{T}^{-1}\vxi{T}\right)
  Y^2  +2 Y\left\vert
  \vxti{T}D_{T}^{-1}\left(\mi+c^{-1}D_{T-1}\right)^{-1}\vei{T-1}-\hat{y}_{T}\right\vert+\hat{y}_{T}^{2}\bigg]~.
\end{align*}
This problem is of a similar form to the one discussed by~\cite{Forster}, from which we get the optimal solution,
\(
\hyi{T} = {\rm clip}\paren{
  \vxti{T}D_{T}^{-1}\left(\mi+c^{-1}D_{T-1}\right)^{-1}\vei{T-1},Y}.
\)
The optimal solution depends explicitly on the bound $Y$, and as its
value is not known, we thus ignore it, and define the output of
the algorithm to be,
\begin{align}
\hyi{T} &=
\vxti{T}D_{T}^{-1}\left(\mi+c^{-1}D_{T-1}\right)^{-1}\vei{T-1}=\vxti{T}D_{T}^{-1}D'_{T-1}\vei{T-1}
~, \label{my_predictor}
\end{align}
where we define
\begin{equation}
D'_{t-1}=\left(\mi+c^{-1}D_{t-1}\right)^{-1}~.
\label{D_prime}
\end{equation}

We call the algorithm \texttt{LASER} for last-step adaptive regressor
algorithm, and it is summarized in \figref{algorithm:laser}.  This algorithm can be
seen also as a forward algorithm~\citep{AzouryWa01}: The predictor of
\eqref{my_predictor} can be seen as the optimal linear {\em model}
obtained over the same prefix of length $T-1$ and the new
input $\vxi{T}$ with fictional-label $\yi{T}=0$. Specifically, from \eqref{e}
we get that if $\yi{T}=0$, then $\mathbf{e}_{T} = \left(\mathbf{I}+c^{-1}\mathbf{D}_{T-1}\right)^{-1}\mathbf{e}_{T-1}$.
The prediction of the optimal predictor defined in \eqref{optimal_Q} is
$\vxti{T}\mathbf{u}_{T}=\vxti{T}\mathbf{D}_{T}^{-1}\mathbf{e}_{T}=\hat{y}_T$,
where $\hat{y}_T$
was defined in \eqref{my_predictor}.
% Clearly, for $c=\infty$ the \texttt{LASER} algorithm reduces to the AAR algorithm~\citep{Vovk01},
%which is also the last step min-max algorithm of~\cite{Forster}.

When the variance $V$ goes to zero, we set
$c=\infty$ and thus we have $D_{t}=b\mi+\sum_{s=1}^t
\vxi{s}\vxti{s}$ used in stationary
algorithms~(\citep{Vovk01,Forster,Hayes,CesaBianchiCoGe05}). In this case
the algorithm reduces to the algorithm by~\cite{Forster}.
%, with the same logarithmic regret bound.
% (note that the last term in \eqref{bound1} is logarithmic in $T$, see the proof of~\cite{Forster}).


%Similar to CR-RLS and ARCOR, this algorithm can be also
%expressed in terms of weight-vector $\vwi{t}$ and a PSD matrix
%$\msigmaii$, by denoting $\vwi{t}=D_{t}^{-1}\vei{t}$
%and $\msigmaii=D_{t}^{-1}$. The algorithm is summarized in
%the middle column of \tabref{table:algorithms}.


\begin{figure}[t!]
{
\paragraph{Parameters:} $0<b<c$
\paragraph{Initialize:} Set
$D_{0}=(bc)/(c-b)\,\mi\in\reals^{d\times d}$ and $\vei0=\vzero\in\reals^d$\\
{\bf For $t=1 \comdots T$} do
\begin{itemize}
\nolineskips
\item Receive an instance $\vxi{t}$
\item Compute
\begin{align*}
  D_{t}=\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}+\vxi{t}\vxti{t}
  \hfill
\end{align*}
\item Output  prediction $\hyi{t}=\vxti{t}D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}$
\item Receive the correct label $\yi{t}$
\item
Update:
\begin{align*}
\vei{t}=\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}+y_{t}\vxi{t}
\hfill
\end{align*}
\end{itemize}
\paragraph{Output:}  $\vei{T} \ ,\ D_{T}$\\
}
\figline
\caption{LASER: last-step adaptive regression algorithm.}
\label{algorithm:laser}
\end{figure}

%
%\begin{figure}[t!]
%{
%\paragraph{Parameters} $0<b<c$
%\paragraph{Initialize} Set
%$D_{0}=(bc)/(c-b)\,\mi\in\reals^{d\times d}$ \\ and %$\vei0=\vzero\in\reals^d$\\
%{\bf For $t=1 \comdots T$} do
%\begin{itemize}
%\nolineskips
%\item Receive an instance $\vxi{t}$
%\item Compute
%%\begin{align*}
%  %$D_{t}=\left(D_{t-1}^{-1}+c^{-1}\mi\right)^{-1}+\vxi{t}\vxti{t}$
%  \hfill
%%\end{align*}
%\item Output  prediction %$\hyi{t}=\vxti{t}D_{t}^{-1}\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}$
%\item Receive the correct label $\yi{t}$
%\item
%Update:
%%\begin{align*}
%$\vei{t}=\left(\mi+c^{-1}D_{t-1}\right)^{-1}\vei{t-1}+y_{t}\vxi{t}$
%\hfill
%%\end{align*}
%\end{itemize}
%\paragraph{Output}  $\vei{T} \ ,\ D_{T}$\\
%}
%\figline
%\caption{LASER: last step adaptive regression algorithm.}
%\label{algorithm:laser}
%\end{figure}


\section{Recursive form}
\label{sec:LASER_recursive}

Similar to the \texttt{WEMM} algorithm, \texttt{LASER} algorithm can also be expressed in a recursive form in terms of weight vector $\mathbf{w}_{t}$ and a covariance-like matrix $\mathbf{\Sigma}_{t}$. We denote $\mathbf{w}_{t}=\mathbf{D}_{t}^{-1}\mathbf{e}_{t}$ and $\mathbf{\Sigma}_{t}=\mathbf{D}_{t}^{-1}$, and develop recursive update rules for $\mathbf{w}_{t}$ and $\mathbf{\Sigma}_{t}$:
\[
\mathbf{\Sigma}_{t}^{-1}=\left(\mathbf{\Sigma}_{t-1}+c^{-1}\mathbf{I}\right)^{-1}+\mathbf{x}_{t}\mathbf{x}_{t}^{\top}~,
\]
\begin{eqnarray*}
\mathbf{w}_{t} & = & \mathbf{D}_{t}^{-1}\mathbf{e}_{t}\\
 & = & \left[\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)^{-1}+\mathbf{x}_{t}\mathbf{x}_{t}^{\top}\right]^{-1}\left[\left(\mathbf{I}+c^{-1}\mathbf{D}_{t-1}\right)^{-1}\mathbf{e}_{t-1}+y_{t}\mathbf{x}_{t}\right]\\
 & = & \left[\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}-\frac{\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)}{1+\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}}\right]\left[\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)^{-1}\mathbf{D}_{t-1}^{-1}\mathbf{e}_{t-1}+y_{t}\mathbf{x}_{t}\right]\\
 & = & \left[\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}-\frac{\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)}{1+\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}}\right]\left[\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)^{-1}\mathbf{w}_{t-1}+y_{t}\mathbf{x}_{t}\right]\\
 & = & \mathbf{w}_{t-1}-\frac{\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}\mathbf{x}_{t}^{\top}\mathbf{w}_{t-1}}{1+\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}}+\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)y_{t}\mathbf{x}_{t}\\
 &&-\frac{\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)}{1+\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}}y_{t}\mathbf{x}_{t}\\
 & = & \mathbf{w}_{t-1}+\frac{\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)y_{t}\mathbf{x}_{t}-\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}\mathbf{x}_{t}^{\top}\mathbf{w}_{t-1}}{1+\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}}\\
 & = & \mathbf{w}_{t-1}+\frac{\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}\left(y_{t}-\mathbf{x}_{t}^{\top}\mathbf{w}_{t-1}\right)}{1+\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}}\\
 & = & \mathbf{w}_{t-1}+\frac{\left(y_{t}-\mathbf{x}_{t}^{\top}\mathbf{w}_{t-1}\right)\left(\mathbf{\Sigma}_{t-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}}{1+\mathbf{x}_{t}^{\top}\left(\mathbf{\Sigma}_{t-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}}
~,
\end{eqnarray*}
and the prediction is
\begin{eqnarray*}
\hat{y}_{t} & = & \mathbf{x}_{t}^{\top}\mathbf{D}_{t}^{-1}\left(\mathbf{I}+c^{-1}\mathbf{D}_{t-1}\right)^{-1}\mathbf{e}_{t-1}\\
 & = & \mathbf{x}_{t}^{\top}\left[\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}-\frac{\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)}{1+\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}}\right]\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)^{-1}\mathbf{D}_{t-1}^{-1}\mathbf{e}_{t-1}\\
 & = & \mathbf{x}_{t}^{\top}\left[\mathbf{I}-\frac{\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}\mathbf{x}_{t}^{\top}}{1+\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}}\right]\mathbf{D}_{t-1}^{-1}\mathbf{e}_{t-1}\\
 & = & \mathbf{x}_{t}^{\top}\left[\frac{1+\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}-\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}\mathbf{x}_{t}^{\top}}{1+\mathbf{x}_{t}^{\top}\left(\mathbf{D}_{t-1}^{-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}}\right]\mathbf{w}_{t-1}\\
 & = & \frac{\mathbf{x}_{t}^{\top}\mathbf{w}_{t-1}}{1+\mathbf{x}_{t}^{\top}\left(\mathbf{\Sigma}_{t-1}+c^{-1}\mathbf{I}\right)\mathbf{x}_{t}}
 ~.
\end{eqnarray*}
A summary of the algorithm in a recursive form appears in the right column of \tabref{table:non_stat_algorithms}.


\section{Comparison to other algorithms}
\label{sec:LASER_comparison}

\begin{center}
\begin{table*}[t]
{\tiny
\hfill{}
\begin{tabulary}{1.26\textwidth}{|C|C|L|L|L|} % centered columns (5 columns)
\hline                       %inserts double horizontal lines
 &  & \bf ARCOR & \bf CR-RLS   & \textbf{LASER} (this work) \\ [0.5ex] % inserts table
%heading
\hline                  % inserts single horizontal line
 Parameters  & & $0<\cor,R_B$ ~,~ a sequence $1 > \Lambda_1 \geq
\Lambda_2 ... $ & $0<\cor\leq1, T_0\in\mathbb{N}$ & $0<b<c$ \\ % inserting body of the table
\hline
 Initialize &  & $\vwi{0}=0 ~,~ \msigmai{0}=\mi ~,~ i=1$ &
 $\vwi{0}=0 ~,~ \msigmai{0}=\mi$ & $\vwi{0}=0 ~,~ \msigmai{0}=\frac{c-b}{bc}\mi$ \\ [0.5ex]
\hline
%\cline{2-5}
 & & \multicolumn{3}{c|}{ Receive an instance $\vxi{t}$}  \\
\cline{2-5}
\vspace{1.5cm} For $t=1 ... T$  & \vspace{0.5cm} Output prediction: & \[\hyi{t}=\vxti{t}\vwi{t-1}\] & \[\hyi{t}=\vxti{t}\vwi{t-1}\]  & \[\hyi{t}=\frac{\vxti{t}\vwi{t-1}}{1+\vxti{t}\paren{\msigmai{t-1}+c^{-1}\mi}\vxi{t}}\] \\
\cline{2-5}
 & & \multicolumn{3}{c|}{Receive a correct label $\yi{t}$ }  \\
 \cline{2-5}
 & \vspace{0.5cm} Update $\msigmai{t}$: & \[\tilde{\msigma}_{t}^{-1} = \msigmai{t-1}^{-1} + \frac{1}{\cor}\vxi{t}\vxti{t}\] & \[\tilde{\msigma}_{t}^{-1} = \cor\msigmai{t-1}^{-1} + \vxi{t}\vxti{t}\] & \[\msigma_{t}^{-1} = \paren{\msigmai{t-1}+c^{-1}\mi}^{-1}
+ \vxi{t}\vxti{t}\] \\
 & & \begin{tabular}{lll}
If &$\tilde{\msigma}_t \succeq \Lambda_i \mi$\\
  &~~set $\msigmai{t}  = \tilde{\msigma}_t$\\
 else&~~set $\msigmai{t} =
  \mi~,~ i = i+1$% ~,~ \Lambda_i^{-1} = i^q+1$
\end{tabular} &
\begin{tabular}{lll}
If &$\mod(t,T_0)>0$\\
        &~~set $\msigmai{t}  = \tilde{\msigma}_t$\\
 else&~~set $\msigmai{t} =
  \mi$% ~,~ \Lambda_i^{-1} = i^q+1$
\end{tabular} & \\
\cline{2-5}
 & \vspace{0.5cm} Update $\vwii$: & \[\begin{array}{ll}\tvwi{t}  &\!\!\!\!=
   \vwi{t-1}\\&\!\!\!\!+\frac{(\yi{t}-\vxti{t}\vwi{t-1})\msigmai{t-1}\vxi{t}}{\cor+\vxti{t}\msigmai{t-1}\vxi{t}}\\~\\\vwii &\!\!\!\!= \proj\paren{{\tvwi{t}},\msigmaii, \rb}\end{array}\]
& \[\begin{array}{ll}\vwi{t}  &\!\!\!\!= \vwi{t-1}\\&\!\!\!\!+\frac{(\yi{t}-\vxti{t}\vwi{t-1})\msigmai{t-1}\vxi{t}}{\cor+\vxti{t}\msigmai{t-1}\vxi{t}}\end{array}\]
 & \[\!\!\!\!\!\!\begin{array}{ll}\vwi{t}  &\!\!\!\!\!= \vwi{t-1}\\&\!\!\!\!\!+\frac{(\yi{t}-\vxti{t}\vwi{t-1})\paren{\msigmai{t-1}+c^{-1}\mi}\vxi{t}}{1+\vxti{t}\paren{\msigmai{t-1}+c^{-1}\mi}\vxi{t}}\end{array}\] \\
 \hline
Output & & \multicolumn{3}{c|}{ $\mathbf{w}_{T} \ ,\ \mathbf{\Sigma}_{T}$ } \\
 [1ex]     % [1ex] adds vertical space
\hline  %inserts single line
\end{tabulary}}
\caption{Second order online algorithms for non-stationary regression} % title of Table
\hfill{}
\label{table:non_stat_algorithms}
\end{table*}
\end{center}

In this section we compare similar second order online algorithms for non-stationary regression. The CR-RLS algorithm~\citep{Salgado,Goodwin,Chen},
summarized in the middle column of \tabref{table:non_stat_algorithms}, is a variant of the RLS algorithm for non-stationary environment. The algorithm performs a step called covariance-reset, which resets the second-order information every fixed amount of rounds. This step enables the algorithm not to converge or get stuck, which is an essential property in non-stationary environment. The only difference of CR-RLS from RLS is that after updating the matrix $\msigmai{t}$, the
algorithm checks whether $T_0$ (a predefined natural number) examples
were observed since the last restart, and if this is the case, it sets
the matrix to be the identity matrix. Clearly, if $T_0=\infty$ the CR-RLS algorithm is reduced to the RLS algorithm.

The ARCOR algorithm~\citep{VaitsCr11},
summarized in the left column of \tabref{table:non_stat_algorithms}, is a variant of the AROWR algorithm~\citep{VaitsCr11,CrammerKuDr12} for non-stationary environment. The algorithm is similar to CR-RLS algorithm but it resets the covariance matrix each time the smallest eigenvalue of the covariance matrix reaches a certain threshold. Thus, the ARCOR algorithm performs resets based on the actual properties of the data: the eigenspectrum of the covariance matrix.

\tabref{table:non_stat_algorithms} enables us to compare the three algorithms head-to-head. All algorithms perform linear predictions, and then update the prediction vector $\vwi{t}$ and the matrix $\msigmai{t}$.
CR-RLS and ARCOR are more similar to each other, both stem from a stationary algorithm, and perform resets from time-to-time. For CR-RLS it is performed every fixed time steps, while for ARCOR it is performed when the eigenvalues of the matrix (or effective learning rate) are too small. ARCOR also performs a projection step, which is motivated to ensure that the weight-vector will not grow to much. CR-RLS (as well as RLS) also uses a forgetting factor (if $\cor<1$).

Our algorithm, \texttt{LASER}, controls the covariance matrix in a smoother way. On each iteration it interpolates it with the identity matrix before adding $\vxi{t}\vxti{t}$. Note that if $\lambda$ is an eigenvalue of $\msigmai{t-1}^{-1}$ then $\lambda\times\paren{c/(\lambda+c)} < \lambda$ is an eigenvalue of $\paren{\msigmai{t-1} + c^{-1} \mi}^{-1}$. Thus the algorithm implicitly reduce the eigenvalues of the inverse covariance (and increase the eigenvalues of the covariance).

Finally, all three algorithms can be combined with Mercer kernels as they employ only
sums of inner- and outer-products of its inputs. This allows them to perform non-linear predictions, similar to SVM.


