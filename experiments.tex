\chapter{Experiments}

We evaluated the SHAMPO algorithm using four different datasets: USPS, MNIST (both OCR), Vocal Joystick 
(VJ, vowel recognition) and NLP dataset contains document classification and sentiment classification. 
The USPS dataset, contains $7,291$ training examples and $2,007$ test examples, each one of them is a 
$16\times16$ pixels gray-scale images converted to a $256$ dimensional vector. 
The MNIST dataset with $28\times28$ gray-scale images, contains $60,000$ 
examples in the training set and $10,000$ in the testing set. In both cases there are $10$ possible labels,
the $0$ to $9$ digits. The VJ project was built to enable individuals with motor impairments to use vocal
 parameters to control objects on a computer screen (buttons, sliders, etc.) and ultimately 
 electro-mechanical instruments (e.g., robotic arms, wireless home automation devices). The VJ tasks is to 
 predict a vowel from eight possible vowels. Each example is a frame of spoken value described with $13$ 
 MFCC coefficients transformed into 26 features. There are $572,911$ training examples and $236,680$ test 
 examples. In order to allow comparing the the margin between examples, we scaled the example vectors by 
 normalize the vectors. We tried to scale the vectors in to a unit ball, but normalization gives the best performance.
 We also added a constant entry for each one of the vectors to represent the 
 bias element. We created binary tasks from these multi-class datasets using two reductions: One-vs-Rest 
 setting and One-vs-One setting. For example, in both USPS and MNIST there are $10$ binary one-vs-rest 
 tasks and $45$ binary one-vs-one tasks.  The NLP document classification is one-vs-one  binary 
 classification include of spam filtering, news items and news-group classification, sentiment classification, 
 and product domain categorization. A total of $36$ binary prediction tasks over all, with a total of 
 $252,609$ examples, and input dimension varying between $8,768$ and $1,447,866$. 
 Details of the individual binary tasks can be found elsewhere~\cite{Crammer:2012:CLC:2343676.2343704}.
Since the NLP dataset doesn't have a specific test set, we evaluated the algorithm performance using 
10 folds cross validations. This yielded seven collections (USPS, MNIST and VJ\@; each as one-vs-rest or one-vs-one) and document 
classification.

\section{Hard and Easy Tasks Tradeoff}
First, we wanted to show that indeed, the SHMAPO algorithms are optimal for different collections of hard and
 easy tasks, and also for tasks from different domains and dimensions. For that reason, we created an eighth 
 collection, named MIXED, which consists of $40$ tasks: $10$ random tasks from each one of the four 
 basic datasets (one-vs-one versions). Then, from each one of the eight dataset collections we generated 
 between $6$ to $10$ combinations (or problems), each problem was created by sampling between 
 $2$ and $8$ tasks which yielded a total of $64$ multi-task problems. 
We tried to diversify problems difficulty by including both hard and easy binary classification problems.
For example, from the VJ one-vs-one dataset we generated 10 multitask problems with up to 5 tasks each, and different 
number of hard and easy tasks : 1 (easy task) + 1 (hard task),1+3, 1+5, 1+7, 3+1, 3+3, 3+5, 5+1, 5+3, 7+1.
The hardness of a binary problem is evaluated by the number of mistakes the Perceptron algorithm performs 
on each task, where for each multitask problem, we sample uniformly from each group of tasks 
(hard and easy). The exact details on the sub data sets collection are collected 
in \tabref{tab:collections}. The max tasks columns indicates the size of the largest sub multitask collection 
for a certain dataset. The number of easy and hard group number is the size of 
the group of tasks from which we sample the hard or easy task. The last column 
is the number of multitask problems collections that were generated for the dataset (which sums up to 64) . 
For example, from the data set VJ one-vs-Rest we generated 6 multitask problems, 
with at most 5 tasks for a single multitask problem, when the easy tasks were 
sampled from the first 4 easier tasks and the harder tasks were sampled from the 
next 4 tasks, which are harder. It is important to point out that since we 
sample the data sets (for some datasets, we don't count the hardest tasks), we should not compare the 
results of this experiment with the results of experiments on the whole dataset tasks.

\begin{table}[h]
\centering
\caption{The data subset collections details}
\label{tab:collections}
\begin{tabular}{|l|c|c|c|c|}
\hline
Dataset & \multicolumn{1}{l|}{Max Tasks} & \multicolumn{1}{l|}{Easy group \#} & \multicolumn{1}{l|}{Hard group \#} & \multicolumn{1}{l|}{Collections} \\ \hline
VJ 1 vs 1 & 8 & 10 & 10 & 10 \\ \cline{1-1}
VJ 1 vs Rest & 5 & 4 & 4 & 6 \\ \cline{1-1}
USPS 1 vs 1 & 8 & 20 & 20 & 10 \\ \cline{1-1}
USPS 1 vs Rest & 6 & 5 & 5 & 6 \\ \cline{1-1}
MNIST 1 vs 1 & 8 & 10 & 10 & 10 \\ \cline{1-1}
MNIST 1 vs Rest & 6 & 5 & 5 & 6 \\ \cline{1-1}
NLP documents & 6 & 8 & 8 & 6 \\ \cline{1-1}
MIXED & 8 & 10 & 10 & 10 \\ \hline
\end{tabular}
\end{table}


We evaluated two baselines in addition to our algorithm. Algorithm {\em uniform} picks a random task to be 
queried and updated (corresponding to $b\rightarrow\infty$), {\em exploit} which picks the tasks with the 
lowest absolute margin (i.e. the ``hardest instance''), this combination corresponds to $b \approx 0$ of 
SHAMPO.\@ We tried for SHAMPO $13$ values for $b$, equally spaced on a logarithmic scale between 
$10^{-7}$ and $10^{5}$. 
All algorithms made a single pass over the training data. We ran two versions of our First Order algorithm: 
plain version, without aggressiveness (updates on mistakes only, $\lambda=0$) and an 
Aggressive version $\lambda=b/2$ (we tried lower values of $\lambda$ as in the bound, 
but we found that $\lambda=b/2$ gives better results), both with uniform prior ($a_i=1$). 
We used separate training set and a test set, to build a model and evaluate it. We repeated this procedure 
50 times and computed mean test error of all runs on all tasks for each dataset. 

The results are evaluated using $2$ quantities. First, the average test error (over all the dataset combinations) 
and the average score. For each combination we assigned a score of $1$ to the algorithm with the lowest 
test error, and a score of $2$, to the second best, and all the way up to a score of $6$ to the algorithm with 
the highest test error.



\begin{table}[h] 
\begin{centering}
\caption{Test errors percentage . Scores are shown in parenthesis.}
\label{tab:table1}
{\scriptsize	
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
                         & \multicolumn{3}{c|}{\textbf{Aggressive $\lambda=b/2$}}               & \multicolumn{3}{c|}{\textbf{Plain}}                 \\ \hline
\textit{Dataset}         & \textit{exploit} & \textit{SHAMPO}         & \textit{uniform} & \textit{exploit} & \textit{SHAMPO} & \textit{uniform} \\ \hline
{VJ 1 vs 1 }        & 5.22 (2.9)       & \textbf{4.57 (1.1)}   & 5.67 (3.9)       & 5.21 (2.7)       & 6.93 (4.6)    & 6.26 (5.8)       \\
\textrm{VJ 1 vs Rest}    & 13.26 (3.5)      & \textbf{11.73 (1.2)} & 12.43 (2.5)      & 13.11 (3.0)        & 14.17 (5.0)     & 14.71 (5.8)     \\
\textrm{USPS 1 vs 1}      & 3.31 (2.5)       & \textbf{2.73 (1.0)}     & 19.29 (6.0)        & 3.37 (2.5)       & 4.83 (4.0)      & 5.33 (5,0)         \\
\textrm{USPS 1 vs Rest}  & 5.45 (2.8)      & \textbf{4.93 (1.2)}  & 10.12 (6.0)        & 5.31 (2.0)         & 6.51 (4.0)      & 7.06 (5.0)         \\
\textrm{MNIST 1 vs 1}     & 1.08 (2.3)       & \textbf{0.75 (1.0)}     & 5.9 (6.0)         & 1.2 (2.7)       & 1.69 (4.1)      & 1.94 (4.9)     \\
\textrm{MNIST 1 vs Rest} & 4.74 (2.8)      & \textbf{3.88 (1.0)}     & 10.01 (6.0)       & 4.44 (2.8)      & 5.4 (3.8)    & 6.1 (5.0)          \\
\textrm{NLP documents} & 19.43 (2.3)     & \textbf{16.5 (1.0)}     & 23.21 (5.0)        & 19.46 (2.7)     & 21.54 (4.7)  & 21.74 (5.3)     \\
\textrm{MIXED}           & 2.75 (2.4)       & \textbf{2.06 (1.0)}     & 13.59 (6.0)        & 2.78 (2.6)       & 4.2 (4.3)     & 4.45 (4.7)       \\ \hline
\textit{Mean score}      & (2.7)           & \textbf{(1.1)}       & (5.2)           & (2.6)           & (4.3)        & (5.2)           \\ \hline
\end{tabular}
}
\end{centering}
\end{table}

Results  are summarized in \tabref{tab:table1}.  In general, {\em exploit} is better than {\em uniform}
 (for majority of the datasets) and aggressive algorithm is better than non-aggressive one. 
 Aggressive SHAMPO yields the best results both evaluated as average 
(over tasks per combination and over combinations). Remarkably, even in the mixed dataset 
(where tasks are of different nature: images, audio and documents), the aggressive SHAPO improves over 
uniform (4.45\% error) and the aggressive-exploit baseline (2.75\%), and achieves a mean test error of 2.06\%.

Indeed, \tabref{tab:table1} shows that aggressive SHAMPO outperforms other alternatives. Yet, we claim that a 
good prior may improve results. We compute prior over the 45 USPS one-vs-one tasks and 10 
USPS one-vs-rest, by running the perceptron algorithm on $1,000$ examples and computing the number of mistakes.  
We set the prior to be proportional to this number. We then reran aggressive SHAMPO with prior, 
comparing it to aggressive SHAMPO with no prior (i.e. $a_i=1$). 
The results are summarized in \tabref{tab:table2}. The prior improves performance in the USPS 1 vs 1 
collection and USPS 1 vs Rest, when evaluated using score-rank, on averaged it is slightly worse than 
aggressive SHAMPO with not prior.
Aggressive SHAMO with prior achieves average error of $1.47$ (vs. $2.73$ with no prior) on 1-vs-1 USPS 
and $4.97$ (vs $4.93$) on one-vs-rest USPS, with score rank of 1.0 (vs 2.9) and 1.7 (vs 2.0) respectively.
However, \figref{fig:tst_err_u1}  and \figref{fig:tst_err_ur} shows the test error of the non aggressive SHAMPO 
algorithm with prior for all values of $b$ we evaluated. The prior version results does not show a remarkable 
improvement over the plain SHAMPO algorithm. Again, we chose here a specific prior generator system, 
however, another method or another prior knowledge may lead to better results.

 \begin{table}[h]
   \begin{centering}
 \caption{Test errors percentage . Scores are shown in parenthesis.}
 \label{tab:table2}
 {\scriptsize	
 \begin{tabular}{|l|r|r|r|r|r|r|}
 \hline
                         & \multicolumn{3}{c|}{\textbf{Aggressive $\lambda=b/2$}}           & \multicolumn{3}{c|}{\textbf{Aggressive $\lambda=b/2$ with prior}}  \\ \hline
 \textit{Dataset}        & \textit{exploit} & \textit{SHAMPO}     & \textit{uniform} & \textit{exploit} & \textit{SHAMPO}        & \textit{uniform} \\ \hline
 {USPS 1 vs 1}     & 3.31 (3.9)       & 2.73 (2.9)        & 19.29 (5.6)      & 1.92 (2.2)       & \textbf{1.47 (1.0)}    & 17.66 (5.4)      \\
 {USPS 1 vs Rest} & 5.45 (3.5)       & \textbf{4.93 }(2.0) & 10.12 (5.7)     & 5.23 (2.8)      & 4.97 \textbf{(1.7)} & 9.64 (5.3)      \\ \hline
 \end{tabular}
 }
 \end{centering}
 \end{table}
 
 \figref{fig:bars1} and \figref{fig:bars2} show the test error of the three algorithms on two of document 
classification combinations, with four and eight tasks. Three algorithms are evaluated: uniform, exploit, and 
aggressive SHAMPO with $\lambda=b/2$. Clearly, not only SHAMPO performs better, 
but it does so on each task individually. (Our analysis above bounds the total number of mistakes over all 
tasks.)
 
  \begin{figure}[h]
\begin{centering}
\subfigure[4 text classification tasks]{\includegraphics[width=0.45\textwidth]{figs/problem-9_4_2_2-1.eps}\label{fig:bars1}}
\subfigure[8 text classification tasks]{\includegraphics[width=0.45\textwidth]{figs/problem-9_4_4_4-1.eps}\label{fig:bars2}}
 \caption{
 Test error of aggressive SHAMPO on (a) four and (b) eight binary text classification tasks.}
\end{centering}
\end{figure}

\section{Multi-task Binary Classification}

\begin{figure}[p]
\begin{centering}
\subfigure[MNIST one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/train-m1.eps}\label{fig:trn_err_m1}}\hfill
\subfigure[MNIST one-vs-rest]{\includegraphics[width=0.48\textwidth]{figs/train-mr.eps}\label{fig:trn_err_mr}}
\subfigure[USPS one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/train-u1.eps}\label{fig:trn_err_u1}}\hfill
\subfigure[USPS one-vs-rest]{\includegraphics[width=0.48\textwidth]{figs/train-ur.eps}\label{fig:trn_err_ur}}
\subfigure[VJ one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/train-v1.eps}\label{fig:trn_err_v1}}\hfill
\subfigure[VJ one-vs-rest]{\includegraphics[width=0.48\textwidth]{figs/train-vr.eps}\label{fig:trn_err_vr}}
\captcont{Training error - all datasets and algorithms}
\label{fig:train_errors}
\end{centering}
\end{figure}

\begin{figure}[t]
\begin{centering}
\subfigure[NLP one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/train-n.eps}\label{fig:trn_err_n}}
\caption{Training error - all datasets and algorithms (cont.)}
\label{fig:train_errors_2}
\end{centering}
\end{figure}

\begin{figure}[p]
\begin{centering}
\subfigure[MNIST one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/test-m1.eps}\label{fig:tst_err_m1}}\hfill
\subfigure[MNIST one-vs-rest]{\includegraphics[width=0.48\textwidth]{figs/test-mr.eps}\label{fig:tst_err_mr}}
\subfigure[USPS one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/test-u1.eps}\label{fig:tst_err_u1}}\hfill
\subfigure[USPS one-vs-rest]{\includegraphics[width=0.48\textwidth]{figs/test-ur.eps}\label{fig:tst_err_ur}}
\subfigure[VJ one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/test-v1.eps}\label{fig:tst_err_v1}}\hfill
\subfigure[VJ one-vs-rest]{\includegraphics[width=0.48\textwidth]{figs/test-vr.eps}\label{fig:tst_err_vr}}
\captcont{Testing error - all datasets and algorithms}
\label{fig:test_errors}
\end{centering}
\end{figure}

\begin{figure}[t]
\begin{centering}
\subfigure[NLP ove-vs-one]{\includegraphics[width=0.48\textwidth]{figs/test-n.eps}\label{fig:tst_err_n}}
\caption{Testing error - all datasets and algorithms (cont.)}
\label{fig:test_errors_2}
\end{centering}
\end{figure}

\begin{figure}[p]
\begin{centering}
\subfigure[MNIST one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/queried-m1.eps}\label{fig:queried_m1}}\hfill
\subfigure[MNIST one-vs-rest]{\includegraphics[width=0.48\textwidth]{figs/queried-mr.eps}\label{fig:queried_mr}}
\subfigure[USPS one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/queried-u1.eps}\label{fig:queried_u1}}\hfill
\subfigure[USPS one-vs-rest]{\includegraphics[width=0.48\textwidth]{figs/queried-ur.eps}\label{fig:queried_ur}}
\subfigure[VJ one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/queried-v1.eps}\label{fig:queried_v1}}\hfill
\subfigure[VJ one-vs-rest]{\includegraphics[width=0.48\textwidth]{figs/queried-vr.eps}\label{fig:queried_vr}}
\captcont{Training error: queried vs. all - FO algorithm, all datasets}
\label{fig:queried}
\end{centering}
\end{figure}

\begin{figure}[t]
\begin{centering}
\subfigure[NLP one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/queried-n.eps}\label{fig:queried_n}}
\caption{Training error: queried vs. all - FO algorithm, all datasets  (cont.)}
\label{fig:queried_2}
\end{centering}
\end{figure}

\begin{figure}[p]
\begin{centering}
\subfigure[MNIST one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/scatter-m1.eps}\label{fig:scatter_m1}}\hfill
\subfigure[MNIST one-vs-rest]{\includegraphics[width=0.48\textwidth]{figs/scatter-mr.eps}\label{fig:scatter_mr}}
\subfigure[USPS one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/scatter-u1.eps}\label{fig:scatter_u1}}\hfill
\subfigure[USPS one-vs-rest]{\includegraphics[width=0.48\textwidth]{figs/scatter-ur.eps}\label{fig:scatter_ur}}
\subfigure[VJ one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/scatter-v1.eps}\label{fig:scatter_v1}}\hfill
\subfigure[VJ one-vs-rest]{\includegraphics[width=0.48\textwidth]{figs/scatter-vr.eps}\label{fig:scatter_vr}}
\captcont{Queries vs. Testing error - all datasets and algorithms}
\label{fig:test_scatter}
\end{centering}
\end{figure}

\begin{figure}[t]
\begin{centering}
\subfigure[NLP one-vs-one]{\includegraphics[width=0.48\textwidth]{figs/scatter-n.eps}\label{fig:scatter_n}}
\caption{Queries vs. Testing error - all datasets and algorithms (cont.)}
\label{fig:test_scatter_2}
\end{centering}
\end{figure}

First, we evaluated all the mentioned variations of SHAMPO algorithms. We ran a 
single pass over all of the training examples for all datasets and $11$ equally spaced values of b 
between $10^{-5}$ and $10^5$. The algorithms performance was evaluated using 
mean of accumulated training error of all tasks and the mean of test errors over 
all of the tasks for each datasets. Since the NLP dataset doesn't have a test 
set, we evaluated those quantities using 10 cross validation folds. We repeated 
the experiments 100 times and took the mean of all algorithm runs.
To simplify, we denote each one of those algorithms the  
abbreviations: First Order (FO), First Order AGgressive (AG) with two different values of $\lambda$, 
$\lambda = b/2$ and $\lambda = b/4$ , First Order with PRior (PR), First Order 
ADaptive (AD), Second Order (SO) and Second Order AGgressive (SOAG). 
Recall, in the adaptive algorithm the constant $b$ parameter is replaced with the constant 
$\beta$. We added confidence intervals only in the test error figures
\figref{fig:test_errors}, since in the rest experiments the confidence intervals are very small.

\figref{fig:train_errors} shows the mean cumulative training errors over all tasks. The behavior is similar for 
all of the datasets. We can see that as expected, in the exploration interval ($b \gg 1$, since the vector are normalized) 
all the algorithms shows high error rate, where in the vast majority, this is 
also the highest error. In the exploitation interval ($b\rightarrow 0$), we  see 
lower cumulative error than the exploration case for the most of the datasets. 
This makes sense, since we query (and update) more on the harder tasks.
Between those two extreme cases, there is a point where cumulative error is 
lower. This point is where $b$ gets its optimal value. This value can change 
between datasets and algorithms and next we see a convenient experimental way to find it. 
There are algorithms and datasets (e.g. VJ one-vs-one in \figref{fig:trn_err_v1}), that this point converges
into the exploitation area, it may depends on the examples noise and the number of examples 
in the dataset. As we claimed before, a pure exploitation approach may not be the best choice, at least on 
the first online steps, especially for problem with small number of examples.
We can see the same phenomena in \figref{fig:test_errors} which shows the mean 
test error for all tasks and all round for different $b$ values and different 
tasks. This figure shows the actual prediction error on a new unseen data which can help us to estimate our 
performace with respect to other algorithms on the same dataset. Here, we added one more algorithm, the Watch All algorithm (WA), which runs $K$ parallel online - 
perceptron algorithms, one per task, while each one of them query and watches all of the tasks labels, without any limitation on the feedback.
We show this algorithm here as a benchmark, to measure how much SHAMPO algorithms looses or gain 
performance with respect to the algorithm that can watch all the labels. 



\begin{figure}[h]
\begin{centering}
\subfigure[First Order]{\includegraphics[width=0.48\textwidth]{figs/Qr_step_usps1r_FO_1e-3.eps}\label{fig:qr_step_FO}}
\subfigure[Second Order]{\includegraphics[width=0.48\textwidth]{figs/Qr_step_usps1r_SO_1e-3.eps}\label{fig:qr_step_SO}}
\subfigure[Second Order Aggressive]{\includegraphics[width=0.48\textwidth]{figs/Qr_step_usps1r_SOAG_1e-3.eps}\label{fig:qr_step_SOAG}}
\caption{Number of queries vs. time step, USPS one-vs-rest, all tasks}
\label{fig:qr_step}
\end{centering}
\end{figure}

\begin{figure}[h]
\begin{centering}
\subfigure[First Order]{\includegraphics[width=0.48\textwidth]{figs/Qr_Err_usps1r_FO_1e-3.eps}\label{fig:qr_er_FO}}
\subfigure[Second Order]{\includegraphics[width=0.48\textwidth]{figs/Qr_Err_usps1r_SO_1e-3.eps}\label{fig:qr_er_SO}}
\subfigure[Second Order Aggressive]{\includegraphics[width=0.48\textwidth]{figs/Qr_Err_usps1r_SOAG_1e-3.eps}\label{fig:qr_er_SOAG}}
\caption{Number of queries vs. number of mistakes, USPS one-vs-rest, all tasks}
\label{fig:qr_er}
\end{centering}
\end{figure}

\begin{figure}[h]
\begin{centering}
\subfigure[First Order]{\includegraphics[width=0.48\textwidth]{figs/Qr_Err_frac_usps1r_FO_1e-3.eps}\label{fig:qr_er_frac_FO}}
\subfigure[Second Order]{\includegraphics[width=0.48\textwidth]{figs/Qr_Err_frac_usps1r_SO_1e-3.eps}\label{fig:qr_er_frac_SO}}
\subfigure[Second Order Aggressive]{\includegraphics[width=0.48\textwidth]{figs/Qr_Err_frac_usps1r_SOAG_1e-3.eps}\label{fig:qr_er_frac_SOAG}}
\caption{${Queries}/{Mistakes}$, USPS one-vs-rest, all tasks}
\label{fig:qr_er_frac}
\end{centering}
\end{figure}


When we compare all of the algorithms, there is no such absolute winner, however, 
it is obvious from results that the aggressive algorithms shows better 
performance than the rest of them. For most of the algorithms, the aggressive version of the second order (SOAG) seems to preform better 
then the rest. It is probably due to the fact that it draws tasks to query on, based on two measurements 
($r_{i,t} $ and $\hat{p}_{i,t}$) and also make an aggressive update. The second best algorithm is  
the first order aggressive algorithm (AG) which performs the best on a 
limited $b$ interval because the aggressiveness depends strongly on $b$. The 
first order algorithm with prior (PR) doesn't show an improvement over the ordinary first order (FO) algorithm. 
In fact, we already mentioned before that better prior may lead to an improvements in the results. 
The adaptive $b$ algorithm (AD), does not show an improvement over FO as well. 

Now, we focus on the tasks that the algorithm chooses to annotate on each iteration for various values 
of $b$. \figref{fig:queried} shows the total number of mistakes that first order SHAMPO  algorithm 
made during training time on all datasets. 
We show here two quantities: fraction of mistakes over all training examples (denoted by ``All tasks'' - blue) 
and fraction of mistakes over only queried examples (denoted by ``Queried tasks'' - dashed red). 
In pure exploration (large values of $b$) both quantities are the same, as the choice of task to be labeled 
is independent of the task and example, and essentially the fraction of mistakes in queried examples is a 
good estimate of the fraction of mistakes over all examples. 
The other extreme is when performing pure exploitation (low values of
$b$), here, the fraction of mistakes made on queried examples went up, while the overall fraction of mistakes 
went down. This indicates that the algorithm indeed focuses its queries on the harder inputs, which in turn, 
improves overall training mistake. There is a sweet point of $b$ (that is changed between datasets), for 
which SHAMPO is still focusing on the harder examples, yet reduces the total fraction of training mistakes even more. 
The existence of such tradeoff is predicted by \thmref{thm:FO_theorem}.  
So far we saw algorithms and bounds that depends on the value 
of $b$ and proof that an optimal value 
that get a minimal accumulative train error exists. The inevitable question is: how to find such 
optimal value? We suggested an adaptive algorithm, yet, this algorithm have an 
initial value to set. Is there a practical way to find this value? Recall, the only prediction mistakes that the algorithm knows 
about, are the mistakes of the queried tasks. When we look at the accumulative mistakes that the algorithm 
made in  \figref{fig:queried}, it is possible to find an answer for this question.
The value of $b$ which minimizes the mean test error, is about the point for which there is a change in the error of 
queried examples (the only quantity SHAMPO observes), which provides a rough rule-of-thumb to pick 
$b$ automatically on-the-fly.


Another perspective of the phenomena is that for values of $b\ll 1$ SHAMPO focuses on the harder 
examples, is illustrated in \figref{fig:test_scatter} where test error vs number of queries is plotted for each 
task of all the datasets. We show three cases: uniform, exploit and a mid-value, the optimal one, which 
tradeoffs exploration and exploitation. We take the MNIST one-vs-one data set as an example and 
refer a few points. First performing uniform querying, all 
tasks have about the same number of queries ($266$), close to the number of examples per problem 
($12,000$), divided by the number of problems ($45$). Second, when having a tradeoff between exploration 
and exploitation ($b\approx 0.01$) , harder problems (as indicated by test error) get more queries than easier problems. 
For example, the four problems with test error greater than $6\%$ get at least $400$ queries, which is 
about twice the number of queries received by each of the $10$ problems with test error less than $1\%$. 
Third, as a consequence, SHAMPO performs equalization, giving the harder problems more labeled data, 
and as a consequence, reduces the error of these problems, however, is not increasing the error of the 
easier problems which gets less queries (in fact it reduces the test error of almost all 45 problems compare 
to the exploration method!). 
The tradeoff mechanism of SHAMPO, reduces the test error of each problem by more than $40\%$ 
compared to full exploration. Fourth, exploits performs similar equalization, yet in some hard tasks it 
performs worse than SHAMPO. This could be because it overfits the training data, by focusing on 
hard-examples too much, as SHAMPO has a randomness mechanism.

In the past experiments, we concentrated on the final state of the algorithms: 
the training or testing mistakes at the end of the learning process. We used 
this final state to explain how the behavior of SHAMPO algorithms can exploit the use of the annotator and 
manage the queries between tasks in a way that reduces the cumulative mistake ratio. Indeed, the most desired 
property of such algorithm is reducing the cumulative mistake, yet, it is important to
look into the learning process and see how the algorithm controls the queries 
over the tasks and validate our hypothesis and analysis. For this purpose, we chose the USPS one-vs-rest dataset, since it has a small number of 
tasks and examples. We ran a single run of the algorithm and logged the 
queries that each one of the tasks issued, and the prediction mistake of the 
algorithm for each one of the tasks at every time step. We chose to show here the 
results of three algorithms: First Order, Second Order and Second Order Aggressive, in order to show 
different behavior of those algorithms. The algorithms parameter was initialized to $b=10^{-3}$ 
which is close to the optimal value for all of these algorithms, as shown in \eqref{fig:tst_err_ur}

The cumulative number of queries that each task issued up to a certain time step is shown in 
\eqref{fig:qr_step}. The FO algorithm run (\figref{fig:qr_step_FO}) ends with $100-1,400$ queries for each task and alternate 
the preferred task to query (or not to query) on, sharply. There are tasks that 
issued a lot of queries in a short time interval, and then stopped querying for 
another time intervals repeatedly. This shows that this algorithm is 
less stable than the others. The SO algorithm run (\figref{fig:qr_step_SO})  ends with $50-2,000$ queries for each 
task, yet it focuses mainly on two tasks while the rest didn't issue more than $900$ 
queries. The SOAG algorithm (\figref{fig:qr_step_SOAG}) uses the exploration-exploitation remarkably. It 
focuses more on (probably) harder tasks (expitation), since we see that it ends with $550-950$ queries for each 
task. Yet, it does't leave tasks too much time without querying (exploration), so we see smooth curves.

\eqref{fig:qr_er} shows the number of mistakes vs. number of queries. An expected 
behavior of the algorithm is that when the number of mistakes goes up (harder 
task), the number of queries will rise as well. In fact the relation between 
margin and high number of mistakes may not strictly holds for any task and 
dataset. We can see that the FO (\eqref{fig:qr_er_FO}) shows this trend, but it's 
broken in the middle (there are intervals when the mistakes goes up, but no queries are 
issued). This probably shows the intervals when the algorithm focuses on tasks 
that had higher margin at the same time. The same phenomena (even stronger) is seen in the SO algorithm 
(\eqref{fig:qr_er_SO}) since it focuses mainly on two tasks over the others. The SOAG (\eqref{fig:qr_er_SOAG}) 
on the other hand, shows the desired property of SHAMPO algorithm during all of the run time. 

We also evaluated the error-query ratio over time for these algorithms as in 
\eqref{fig:qr_er_frac}. This measurement shows for each task, how many queries 
it issues per error. When this value is too low for one task compare to the other tasks, it means that the algorithm 
issue a lot of queries for the task, even though not many prediction mistakes  
were done on the same task, which is a bad behavior. Saying that, this is not always bad when this 
measure is high. Recall in our setting we have to issue a query at every step, even when the model 
is converged and we don't make many mistakes, one of the tasks have to issue a query, which increases the 
number of queries for the tasks, which in turn, increase this ratio. In this case, we expect an 
exploration, which cause all the tasks to share 
the same, or close error-query ratio. We can see this exact behavior for the SOAG algorithm 
(\eqref{fig:qr_er_frac_SOAG}), when all the tasks show stable values in the 
range ($2.5-5$) queries per mistake. However, low value of this measure is not desired as well, since it 
means that the algorithm makes a lot of mistakes on a certain task, but does not 
issue enough queries on that task, in order to improve the model and decrease the number of 
mistakes. We see in the FO algorithm (\eqref{fig:qr_er_frac_FO}) and even stronger in the SO 
(\eqref{fig:qr_er_frac_SO}) algorithm, that there is a difference between the 
task with the highest value and the task with the lowest value (1-7 for FO and 0.5-10 for 
SO).

In this experiment we saw that even though we want the algorithm to concentrate on 
the difficult tasks, the algorithm, should avoid the attraction to concentrate too much on a specific 
tasks and ignore the rest of them, i.e. an exploitation without an exploration may not be a good 
idea. We also noticed that the Second Order Aggressive algorithm produces high 
performance, not only in the sense of prediction mistakes, but also in the 
exploration-exploitation manner.

 % In fact in the USPS dataset we found a good prior that can improves the performances.  In order to create this prior, we ran over the examples first simple perceptron algorithm with full feedback, and computes the prior as a fraction of the error from the initial run. The results are shown in \tabref{tab:table2} and as.  In addision, the results for the MIXED dataset support the claim that the tasks can comes from different fields and different dimensions.

\subsection{Choosing the Tradeoff Parameter}
In all SHAMPO versions, there is a tradeoff parameter that need to be initialized.  We analyzed the performance 
of all of SHAMPO algorithms on different datasets and shown how the cummulative classification error changes with this
parameter. Yet, there is one inevitable question: how to choose the optimal value?
An optimal parameter $b^*$ is the one that minimize the cummulative error, i.e.
\[
b^* = \argmin{b}{\mathbb{E}\brackets{\sum_{i=1}^{K}\sum_{t=1}^{n}{M_{i,t}}}.
\]

We have shown in \secref{sec:adaptive} a version of an adaptive $b$ parameter that changes 
with time, however, in this case there is different parameter for each task, and
yet, we need to initialize $\beta$ first. 

We discussed already in \thmref {thm:FO_theorem} that the optimal value can't be calculated from the the 
bound, but we can write an optimal value as a function of  $L_{\gamma,n}$ . 
In fact, the cummulative error is unknown during runtime so we have to find an 
empircal way to compute $b$.  Recall the intuition behind SHAMPO algorithm, we 
would like to query on a tasks that related to wrong predictions.
In \figref{fig:queried} we see that for all datasets, the lowest error rate of the queried tasks 
occurs on high values of $b$, which indicates a high error rate on the overall cummulative mistakes 
rate. When we decrease $b$ gradually, we see that the queried error increases 
continiously, and then, stay on a constant value. One can notice 
that the optimal $b$ and it's close vicinity lies in the ''knee''  area of the  queried tasks 
error. Now, the question is how to find this point?
We can initialize our algorithm with high value of $b = b_0$ (pure exploration), a logarithmic step size - $s$ and 
minimal number of examples -  $m$. Then, we wait $m$ iteration, compute 
the mean errors and set $b = b_0/s$,  run the algorithm for $m$ more 
iterations and compute the derivative betewen the last two computed mean errors o  a logarithmic scale. We 
continue with this method  until we find the area where the derivative hit 
$-1$ for the second time, then we keep this value unchanged. The final value of $b$ 
is possibly close to the optimal.


\section{Reduction of Multi-task to Contextual Bandits}


We also evaluated SHAMPO as a contextual bandit algorithm, by breaking a multi-class problem into few 
binary tasks, and integrating their output into a single multi-class problem. 
We focus on the VJ data, as there are many examples, and linear models perform relatively 
well~\cite{lin2009lose}.  We implemented all three reductions
mentioned in \chapref{chap:multiclass}, namely, {\em one-vs-rest}, {\em one-vs-one-random} which picks a 
random label if the feedback is zero, {\em one-vs-one-weak} (which performs updates to increase 
confidence when the feedback is zero), where we set $\eta=0.2$, and the 
Banditron algorithm~\cite{kakade2008efficient}.
%The results are summarized in \figref{fig:simulations}.
All algorithms show the existence of a tradeoff between exploration and exploitation, 
where {\em one-vs-one-random} is most sensitive to the choice of parameters, yet it also achieves the best 
results, for a large range of values for $b$.
The  {\em one-vs-rest} reduction and the Banditron have a test error of about $43.5\%$, and the 
{\em one-vs-one-random} of about $42.5\%$.
Finally, {\em one-vs-one-weak} achieves an error of $39.4\%$.
 This is slightly worse than PLM
~\cite{lin2009lose} with test error of $38.4\%$ (and higher than MLP with $32.8\%$), 
yet all of these algorithms observe only one bit of feedback per example, while both MLP and PLM 
observe $3$ bits (as class identity can be coded with $3$ bits for $8$ classes). We claim that our setting 
can be easily used to adapt a system to individual user, as we only need to assume the ability to recognize 
three words, such as three letters. Given an utterance of the user, the system may ask: 
``Did you say (a) 'a' like 'bad' (b) 'o' like in 'book') (c) none''. The user can communicate the correct answer 
with no need for a another person to key in the answer.

\begin{figure}[!ht]
\begin{centering}
\includegraphics[width=0.7\textwidth]{figs/VJ_three_methods.eps}\label{fig:mc_vj}
\caption{Multi-class error on VJ data for three bandit reductions and the banditron.}
\end{centering}
\end{figure}

\section{SHAMPO Edge Cases}
We discussed about the advantages of the SHAMPO algorithm with a tradoff value that control 
exploration-expoitation and  provided analysis of the cummulative mistake 
bounds. We also supplied experiments that support the intuition.  However, the 
SHAMPO algorithms not always works better than the other options (pure exploration or pure 
exploitation). 

One of the assumptions of  the SHAMPO algorithms is that we can 
compare margins of different tasks. In order to comply with this  assumption 
a scaling of the input vector is required. Yet, when in one or more tasks 
where  the norm of one vector is very big with respect to the majority of the 
vectors. When we scale the vectors in this setting into a ball, we can't longer compare the 
margin, because the margins of the tasks with the she high  norm are much lower 
than the margin of the rest tasks and the SHAMPO algorithms may not work properly. 
This problem can be solved by just a scaling, but normalizing all the input 
vectors onto a unit ball.

Another reason that using the SHAMPO algorithm can lead to a failure, can be 
when an adversary is involved. In the first order SHAMPO for example, when we 
set the $b$ parameter to a low value ($b \rightarrow 0$), the SHAMPO algorithm 
is not stochastic any more and an adversary can expliot this case and choose 
every iterations, examples that don't add information to the algorithm. In order to select an uninformative 
examples, the adversary can choose  examples that are similar to the examples 
that the algorithm got feedback before. As we shown before, using the second order agressive 
SHAMPO with the confidence measure $r_{i,t}$ could help to solve this problem.
 In fact, the adversary can select the 
uninformative example which cause a low margin and in turn will lead to an update, while for the rest 
of the task, the adversary can choose any example, since no update will be done on those 
tasks due to a delta distribution. Another option that the adversary can act, is 
choosing an example that the algorithm will predict a correct prediction on it, 
but will result with a small margin. For the rest of the tasks, the adversary 
can choose examples that lead to wrong prediction (with slightly higher margins), because no update will be 
made on those tasks any way. The adversary can also choose  examples with very high margin for all of the tasks, 
which may decrease the learning rate. 

